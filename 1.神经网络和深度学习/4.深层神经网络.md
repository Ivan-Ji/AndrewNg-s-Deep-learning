<!-- TOC -->

- [4. 深层神经网络](#4-深层神经网络)
    - [4.1 深层神经网络符号定义](#41-深层神经网络符号定义)
    - [4.2 深层神经网络的前向传播（以`Logistic回归`为例）](#42-深层神经网络的前向传播以logistic回归为例)
        - [4.2.1 非向量化的前向传播](#421-非向量化的前向传播)
        - [4.2.2 向量化的前向传播](#422-向量化的前向传播)
    - [4.3 深层神经网络之核对矩阵的维数](#43-深层神经网络之核对矩阵的维数)
        - [总结](#总结)
            - [注意](#注意)
    - [4.4 使用深层表示的原因（相比浅层而言）](#44-使用深层表示的原因相比浅层而言)
    - [4.5 深层神经网络块的构建](#45-深层神经网络块的构建)
    - [4.6 前向和反向传播`Forward and Backward Propagation`](#46-前向和反向传播forward-and-backward-propagation)
    - [4.7 参数&超参数](#47-参数超参数)

<!-- /TOC -->

# 4. 深层神经网络

`Logistic`回归是一个浅层模型，单隐层模型也是一个浅层模型，如果一个模型的隐藏层、隐藏单元比较多，就可以称为深层的神经网络。其实，深层、浅层只是一个程度的问题。

在前几年的人工智能或机器学习社区，人们发现有些函数只有非常深的网络才能学习，而浅一些的模型通常无法学习，虽然处理任何具体问题时都会很难，预先准确判断需要多深的神经网络。所以先试试`Logistic`回归是非常合理的做法：先试一下单层然后两层，接着把隐藏层数量当作一个可以自由选择数值大小的`超参数`，然后在保留交叉验证数据上评估。

## 4.1 深层神经网络符号定义

|![image](4.1-1%20深度神经网络表示图.png)|
|----|
这是一个四层的神经网络（通常不把输入层当作一层，只包含隐藏层和输出层），每层的单元数量为`4,4,3,1`。

这里使用`L`表示神经网络层数（Layers），此时`L=4`。使用 $n^{[l]}$ 表示第 $l$ 层的节点数量，例如在上图中 $n^{[0]}=3,n^{[1]}=5,n^{[2]}=5,n^{[3]}=3,n^{[4]}=1$ 。使用 $a^{[l]}$ 表示第 $l$ 层的激活函数，在`Logistic`回归的前向传播中会有 $a^{[l]}=g(z^{[l]})$ 。使用 $W^{[l]}$ 表示在 $a^{[l]}$ 中计算 $z^{[l]}$ 值的权重， $b^{[l]}$ 也是如此。

## 4.2 深层神经网络的前向传播（以`Logistic回归`为例）

### 4.2.1 非向量化的前向传播

针对`4.1`的深度神经网络图，一个训练样本有如下计算过程：

$\\ z^{[1]}=W^{[1]}x+b^{[1]}\\ a^{[1]}=g(z^{[1]})\\ z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}\\ a^{[2]}=g(z^{[2]})\\ z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}\\ a^{[3]}=g(z^{[3]})\\ z^{[4]}=W^{[4]}a^{[3]}+b^{[4]}\\ a^{[4]}=g(z^{[4]})$

其中， $a^{[4]}$ 就是最后要估算的 $\hat y$ 的值。值得注意的是，`x`在这里也等于 $a^{[0]}$ ，因为输入向量`x`也是第`0`层的激活单元。因此上述计算过程可以有一个如下通式：

$\\ z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\\ a^{[l]}=g(z^{[l]})$

该层的激活函数就是作用到各个`z`，该通式就代表了所有的`Logistic回归`的正向传播。

### 4.2.2 向量化的前向传播

向量化的计算过程和`4.2.1`的计算过程类似，只不过把`z`和`a`以$\begin{bmatrix} z^{(1)} & z^{(2)} & ... & z^{(m)} \end{bmatrix}$的方式堆叠起来构成了矩阵（大写表示向量化），`X`也是等于$A^{[0]}$。因此针对整个训练集的计算进行向量化表示的通式为：

$\\ z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\ A^{[l]}=g(Z^{[l]})$

这里通式和具体计算过程其实可以用一个`for`循环进行表示：

```python
L = 4
for i=1 to L:
    Z[i] = W[i]*A[i] + b[i]
    A[i] = g(Z[i])
```

在实际的计算中，除了使用`for`循环，也没有其他更好的办法去掉显式的`for`。因此在实现正向传播时，用`for`循环也可。

## 4.3 深层神经网络之核对矩阵的维数

检查代码是否有错的方法之一就是核对代码中矩阵的维度。这里以下图为例：

|![image](4.3-1%20核对矩阵维度之神经网络图.png)|
|----|
上图中，L=5，包含`4个隐藏层和1个输出层`。在实现正向传播时，先忽略偏置项`b`同时关注权重`W`，则有公式：$Z^{[1]}=W^{[1]}x$ 。图上的输入层维度为`(2,1)`，第一个隐藏层有3个隐藏单元，而此时的 $Z^{[1]}$ 是第一个隐藏层的激活函数的向量，它的维度为`(3,1)`，因此这里的`W`就需要满足自身维度乘以输入层维度后等于 $Z^{[1]}$ 的维度。根据线性代数中的矩阵乘法法则，这里的 $W^{[1]}$ 的维度就是`(3,2)`（不熟悉线性代数的同学可以学习以下相关知识）。

接着看偏置项`b`。$b^{[1]}$ 是一个`(3,1)`的向量，因为做向量加法时，必须是维度相同的向量才能相加。

### 总结

在类似上图的`Logistic`回归中，$W^{[l]}$ 的维度必须是 $(n^{[l]},n^{[l-1]})$，$b^{[l]}$ 的维度必须是 $(n^{[l]},1)$。而在实现反向传播的时候，$dW^{[l]}$ 和 $W^{[l]}$ 的维度应该相同，$db^{[l]}$ 和 $b^{[l]}$ 的维度也是相同的。相应地，$z^{[l]},a^{[l]},dz^{[l]},da^{[l]}$ 的维度都是 $(n^{[l]},1)$。

#### 注意

Z、A、X的维度会在向量化后发生变化。上图的`x1、x2`只是其中的一个训练样本，当 $X=\begin{bmatrix} x^{(1)} & x^{(2)} & ... & x^{(m)} \end{bmatrix}$，即`X`的维度变为 $(n^{[0]},m)$ 时，$Z^{[1]}$的维度不再是 $(n^{[1]},1)$，而是变成了 $(n^{[1]},m)$。$W^{[l]}$ 的维度不变。其他变量的维度与本节`4.3`总结的维度差距就在把 $(n^{[l]},1)$ 变为了 $(n^{[l]},m)$。

## 4.4 使用深层表示的原因（相比浅层而言）

神经网络需要比较多的隐藏层。用以下几个例子帮助说明原因：

1. 神经网络究竟在计算什么。神经网络较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来去计算更复杂的东西，这和人类大脑的运作机制类似。

   <blockquote>如果建一个人脸检测系统，神经网络所做的事就是：当你输入一张人脸的照片，然后把神经网络的第一层当成一个特征探测器（或边缘探测器），接着把组成边缘的像素放在一起看，这些像素可以把被探测到的边缘组成面部的不同部分。例如，可能有一个神经元去找眼睛的部分，另外几个在找鼻子的部分。把这些边缘组合在一起，就可以开始检测人脸的不同部分。再把这些部分放在一起，就可以识别不同的人脸了。这种神经网络被称为`CNN 卷积神经网络`。因此直觉上可以把`CNN`的前几层当作探测简单的函数，比如边缘，之后把他们和后几层结合在一起，总体上就能学习更多复杂的函数。这些知识将会在`CNN 卷积神经网络`的章节学习到。</blockquote>

2. 电路理论`circuit theory`。神经网络和基于不同基本逻辑门（与门、或门、非门）的电路元件计算哪些函数有着分不开的联系。在非正式情况下，这些函数都可以用相对小（隐藏单元数量少）但很深的神经网络计算。但如果用浅一些的神经网络去计算同样的函数，就需要成指数增长的单元数量才能达到同样的计算结果。如下为电路理论的通俗解释：

   <blockquote>假设要对输入特征(x1,x2,x3,...xn)计算“异或”或者“奇偶性”，技术上只用“或门+非门”才能计算“异或”函数，用相对小的电路就可以计算“异或”了。然后继续建立“异或”的树图，就可以得到输入特征的“异或”或“奇偶性”。此时树图对应网络的深度为O(log(n))，节点的数量和电路部件、门的数量不会很多，此时不需要太多“门”去计算“异或”，本例中隐藏层数量为O(log(n))。但如果不能使用多隐层神经网络的话，假设使用单隐层计算，只能被迫让单隐层单元的数量指数级增加，这里是2的n-1次方，也就是O(2^n)。</blockquote>

## 4.5 深层神经网络块的构建

这里使用前向传播和反向传播建立一个深度神经网络。以下图为例：

|![image](4.5-1%20深层神经网络构建示意图.png)|
|----|
这是一个层数较少的神经网络，其构建步骤如下：

1. 正向传播

   选择其中一层，在第`l`层有参数 $W^{[l]},b^{[l]}$，正向传播里有输入的激活函数，前一层的激活函数（即本层的输入）为 $a^{[l-1]}$，本层的激活函数（即本层的输出）为$a^{[l]}$。之前的公式

   $\\ z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\\ a^{[l]}=g^{[l]}(z^{[l]})$

   就是从输入 $a^{[l-1]}$ 走到输出 $a^{[l]}$ 的。之后可以把 $z^{[l]}$ 的值作为缓存`cache`，因为缓存的 $z^{[l]}$ 对后面的正向传播、反向传播步骤非常有用。
2. 反向传播
   跟正向传播一样计算第 $l$ 层。输入为 $da^{[l]}$ 和正向传播中缓存的 $z^{[l]}$，输出为 $da^{[l-1]}$ 和需要的梯度 $dW^{[l]},db^{[l]}$。具体计算公式如下：

   $dz^{[l]}=da^{[l]}*g^{[l]'}(z^{[l]})\\ dw^{[l]}=dz^{[l]}a^{[l-1]}\\ db^{[l]}=dz^{[l]}\\ da^{[l-1]}=w^{[l]T}dz^{[l]}$

经过这两步，在第`l`层就有如下的基本计算单元，其中红色代表反向传播的过程。

|![image](4.5-2%20深层神经网络基本计算单元.png)|
|----|

**总结**：神经网络一个梯度下降循环的计算，是由输入`x`即 $a^{[0]}$ 开始，经过一系列正向传播计算得到 $\hat y$，利用输出值 $\hat y$ 计算出 $da^{[L]}$，再实现反向传播，得到所有的导数项，参数`w`也会在每一层里被更新为 $w = w-\alpha dw$，参数`b`也是如此。其计算示意图如下所示：

|![image](4.5-3%20深层神经网络梯度下降循环示意图.png)|
|----|

## 4.6 前向和反向传播`Forward and Backward Propagation`

非向量化版本的公式在前面有推导和展示，而向量化版本的前向和反向传播公式如下所示：

$Forward\,Propagation:\\ \,\, input\,A^{[l-1]}\\ \,\, output\,A^{[l]},cache\,Z^{[l]}:\\ \,\,\,\,\,\, Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\ \,\,\,\,\,\, A^{[l]}=g^{[l]}(Z^{[l]})$

$Backward\,Propagation:\\ \,\, input\,dA^{[l]} \\ \,\, output\,dA^{[l-1]},dW^{[l]},db^{[l]} : \\ \\ \,\,\,\,\,\, dZ^{[l]}=dA^{[l]}*{g^{[l]}}'(Z^{[l]}) \\ \,\,\,\,\,\, dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T} \\ \\ \,\,\,\,\,\, db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},\,axis=1,\,keepdims=True) \\ \\ \,\,\,\,\,\, dA^{[l-1]}=W^{[l]T}dZ^{[l]}$

在这些步骤中，使用到了递归：前向传播时使用输入数据`x`进行初始化，反向递归使用 $da^{[L]}$ 进行初始化（向量化时横向堆叠每个实例）。

## 4.7 参数&超参数

逻辑回归中的参数为 $W^{[1]},b^{[1]},W^{[2]},b^{[2]}......$。但是在神经网络中，有一些参数需要预先设置好，被称为`超参数 Hyper Parameters`。超参数大致有以下几种：

```text
学习率 learning rate： α
迭代次数（梯度下降循环的数量） iterations
隐藏层数量 hidden layers： L
隐藏单元数 hidden units： n^[1],n^[2]...
激活函数 activation function： ReLU,tanh,sigmoid...
```

超参数在某种程度上决定了最终得到的`W`和`b`。
momentum、mini batch、几种不同的正则化参数等超参数将会在之后的章节中学习到。
