<!-- TOC -->

- [3. 浅层神经网络](#3-浅层神经网络)
    - [3.1 神经网络的表示](#31-神经网络的表示)
    - [3.2 计算神经网络的输出（以Logistic回归为例）](#32-计算神经网络的输出以logistic回归为例)
    - [3.3 多个例子中的向量化](#33-多个例子中的向量化)
        - [3.3.1 向量化实例](#331-向量化实例)
        - [3.3.2 向量化实现的解释](#332-向量化实现的解释)
            - [手动计算正向传播](#手动计算正向传播)
    - [3.4 不同的激活函数（activation function）`可以是线性的，也可以是非线性的，不同层的激活函数可以不同`](#34-不同的激活函数activation-function可以是线性的也可以是非线性的不同层的激活函数可以不同)
        - [3.4.1 `σ`函数](#341-σ函数)
        - [3.4.2 `tanh`函数（双曲正切函数）](#342-tanh函数双曲正切函数)
        - [3.4.3 `ReLU`函数（Rectified Linear Unit）`又称线性修正单元`](#343-relu函数rectified-linear-unit又称线性修正单元)
    - [3.5 神经网络需要非线性激活函数的原因](#35-神经网络需要非线性激活函数的原因)
    - [3.6 基本激活函数的导数](#36-基本激活函数的导数)
        - [3.6.1 `sigmoid`函数](#361-sigmoid函数)
        - [3.6.2 `tanh`函数](#362-tanh函数)
        - [3.6.3 `ReLU`函数](#363-relu函数)
        - [3.6.4 `Leaky ReLU`函数(带泄露的`ReLU`函数)](#364-leaky-relu函数带泄露的relu函数)
    - [3.7 神经网络的梯度下降法(单隐层)](#37-神经网络的梯度下降法单隐层)
    - [3.8 随机初始化参数权重](#38-随机初始化参数权重)
        - [解决办法：随机初始化所有参数](#解决办法随机初始化所有参数)

<!-- /TOC -->

# 3. 浅层神经网络

在神经网络中，通常使用方括号 $x^{[i]}$ 表示第`i`层网络的数据，使用圆括号 $x^{(i)}$ 表示第`i`个数据。

## 3.1 神经网络的表示

通常神经网络图一般表示如下：

|![image](3.1-1%20神经网络表示图.png)|
|----|

其中，输入特征 $x_{1},x_{2},x_{3}$ 为`输入层`， $\hat{y}$ 为`输出层`，中间的神经网络层为`隐藏层`。输入层、输出层可能有一个值，也有可能有多个值。

输入特征也可以表示为 $a^{[0]}$ ，其中`a`表示`activations(激活)`，意味着网络中不同层的值传递给后面的网络层。

输入层将`x`的值传递给隐藏层，输入层的激活值表示为 $a^{[0]}$ 。隐藏层同样会产生激活值，其中第一层隐藏层记作 $a^{[1]}$ ，第一个节点记作 $a^{[1]}_{1}$ ，第二个节点记作 $a^{[1]}_{2}$ ，以此类推。在本节所示图中则可以表示为 $a^{[1]}=\begin{bmatrix} a^{[1]}_{1}\\a^{[1]}_{2}\\a^{[1]}_{3}\\a^{[1]}_{4} \end{bmatrix}$ ，它是一个`4*1`的矩阵，或者称为大小为`4`的列向量。第二层隐藏层 $a^{[2]}$ 的值就是 $\hat{y}$ 的值。这和`Logistic回归`类似。

当计算神经网络的层数时，通常不算`输入层`，原因是：隐藏层是第一层，输出层是第二层，在定义中，将输入层称为`第0层`，因此字面上可以不算输入层。本节所示图可以被称作`双层神经网络`。

其实，隐藏层和输出层都是带有参数的。这些参数将在后续章节进行讨论。

## 3.2 计算神经网络的输出（以Logistic回归为例）

以Logistic回归的计算为例进行讲解，计算过程如图所示：

|![image](3.2-1%20回归计算的步骤.png)|
|----|
这里的圆圈代表了回归计算的两个步骤：首先按照步骤计算出`z`的值，接着计算激活函数`σ`的值。神经网络只是重复计算这些步骤很多次。
回顾`3.1 神经网络的表示`中的图，这里直接搬过来。

|![image](3.1-1%20神经网络表示图.png)|
|----|
① 先看第一个隐藏层节点，这个节点要进行回归的两步计算。先计算 $z^{[1]}_{1}=w^{[1]T}_{1}x+b^{[1]}_{1}$ ，再计算 $a^{[1]}_{1}=\sigma (z^{[1]}_{1})$ 。因为是在神经网络里，所以这里要使用上标和下标。

② 第二个隐藏层节点也是如此，先计算 $z^{[1]}_{2}=w^{[1]T}_{2}x+b^{[1]}_{2}$ ，再计算 $a^{[1]}_{2}=\sigma (z^{[1]}_{2})$ 。第三个和第四个隐藏层节点以此类推。计算结果如下图所示：

|![image](3.2-2%20回归计算的等式.png)
|----|

③ 将步骤二中的等式向量化。先将`w`构成矩阵， $\begin{bmatrix} -w^{[1]T}_{1}-\\ -w^{[1]T}_{2}-\\ -w^{[1]T}_{3}-\\ -w^{[1]T}_{4}- \end{bmatrix}$ ，乘以特征向量`x`后 $\begin{bmatrix} -w^{[1]T}_{1}-\\ -w^{[1]T}_{2}-\\ -w^{[1]T}_{3}-\\ -w^{[1]T}_{4}- \end{bmatrix}x$ ，再加上常数项`b`可以得到 $z^{[1]}$ ，然后接上`σ`函数就能得到 $a^{[1]}=\sigma(z^{[1]})$ 。（这里的`x`展开后为 $\begin{bmatrix} x_{1}\\ x_{2}\\ x_{3} \end{bmatrix}$ ）。

④ 进行下一层网络（即输出层）的计算。如下列的后两行公式：

$\\ z^{[1]}_{1}=w^{[1]T}_{1}x+b^{[1]}_{1}......{(1)}\\ a_{1}^{[1]}=\sigma(z_{1}^{[1]})............... {(2)} \\ z^{[2]}_{2}=w^{[2]T}_{2}a_{1}^{[1]}+b^{[2]}_{2}...{(3)}\\ a_{2}^{[2]}=\sigma(z_{2}^{[2]})............... {(4)}$

## 3.3 多个例子中的向量化

### 3.3.1 向量化实例

引用3.2中的公式，如下图所示：

$\\ z^{[1]}_{1}=w^{[1]T}_{1}x+b^{[1]}_{1}......{(1)}\\ a_{1}^{[1]}=\sigma(z_{1}^{[1]})............... {(2)} \\ z^{[2]}_{2}=w^{[2]T}_{2}a_{1}^{[1]}+b^{[2]}_{2}...{(3)}\\ a_{2}^{[2]}=\sigma(z_{2}^{[2]})............... {(4)}$

不使用向量化时：对于输入向量`x`，如果它有`m`个样本，就需要使用一个`for`循环：`for i in m:`，执行`m`次，来执行上面所有的步骤，表示所有样本都执行了计算。如下图：

$for \, i=1\,\, to\,\, m: \\ \,\,\,\, z^{[1]}_{1}=w^{[1]T}_{1}x+b^{[1]}_{1} \\ \,\,\,\, a_{1}^{[1]}=\sigma(z_{1}^{[1]}) \\ \,\,\,\, z^{[2]}_{2}=w^{[2]T}_{2}a_{1}^{[1]}+b^{[2]}_{2} \\ \,\,\,\, a_{2}^{[2]}=\sigma(z_{2}^{[2]})$

使用向量化时：令输入特征 $X=\begin{bmatrix} | & | & | & |\\ x^{(1)} & x^{(1)} & ... & x^{(m)}\\ | & | & | & | \end{bmatrix}$ ，它是一个`(n_x,m)`维的向量，把输入特征`X`带入如下公式中即可

$\\ Z^{[1]}=W^{[1]}X+b^{[1]}\\ A^{[1]}=\sigma(Z^{[1]})\\ Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\ A^{[2]}=\sigma(Z^{[2]})$

值得注意的是， $Z^{[1]},A^{[1]},Z^{[2]},A^{[2]}$ 的结构和输入特征`X`的特征是类似的，都是`(n,m)`维的向量。
横向向量代表了不同的训练样本，训练样本之间使用一定的排序指标进行排序。因此在这里，从左向右扫的时候，就扫过了整个训练集。纵向指标对应了神经网络的不同节点，表示第`x`个向量的第`y`个隐藏单元。

### 3.3.2 向量化实现的解释

#### 手动计算正向传播

1.计算`z`。首先计算 $z^{[1](1)}=w^{[1]}x^{(1)}+b^{[1]},z^{[1](2)}=w^{[1]}x^{(2)}+b^{[1]},z^{[1](3)}=w^{[1]}x^{(3)}+b^{[1]}$ 前三个样本。令`b=0`，这三个公式就变为 $z^{[1](1)}=w^{[1]}x^{(1)},z^{[1](2)}=w^{[1]}x^{(2)},z^{[1](3)}=w^{[1]}x^{(3)}$ 。因为 $w^{[1]}=\begin{bmatrix} ---\\ ---\\ ---\\ --- \end{bmatrix}$ ，因此 $w^{[1]}x^{(1)}=\begin{bmatrix} .\\ .\\ .\\ . \end{bmatrix}$ ，结果是一个列向量，同样的 $w^{[1]}x^{(2)}=\begin{bmatrix} .\\ .\\ .\\ . \end{bmatrix},w^{[1]}x^{(2)}=\begin{bmatrix} .\\ .\\ .\\ . \end{bmatrix}$ 也是列向量。考虑到整个训练集 $X$ ，令 $X=\begin{bmatrix} | & | & |\\ x^{(1)} & x^{(2)} & x^{(3)}\\ | & | & | \end{bmatrix}$ ，则 $W^{[1]}X=W^{[1]}\begin{bmatrix} | & | & |\\ x^{(1)} & x^{(2)} & x^{(3)}\\ | & | & | \end{bmatrix}=\begin{bmatrix} .&.&.\\ .&.&.\\ .&.&.\\ .&.&. \end{bmatrix}=\begin{bmatrix} |&|&|\\ z^{[1](1)}&z^{[1](2)}&z^{[1](3)}\\ |&|&| \end{bmatrix}\\ = Z^{[1]}$ ，即列向量表示的$z$。有更多样本也是如此，只是有更多的列数。之后`b`会加到矩阵各列。

2.后续计算同步骤 1 。

## 3.4 不同的激活函数（activation function）`可以是线性的，也可以是非线性的，不同层的激活函数可以不同`

### 3.4.1 `σ`函数

在之前的介绍里，一直用的是σ函数作为激活函数。σ函数的公式为：$a=\frac{1}{1+e^{-z}}$，结果介于0和1之间。它的导数为 $σ(x)⋅(1−σ(x))=a(1-a)$ 。它的图像如下图所示:

|![image](3.4.1%20σ函数图像.jpg)|
|----|
缺点：`z`特别大或特别小的时候，导数的梯度（函数的斜率）会很小，拖慢梯度下降算法。

### 3.4.2 `tanh`函数（双曲正切函数）

`tanh`函数的表现总比`σ`函数要好，它的函数表达式为：$a=tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}=2\sigma(2z)-1$，结果介于-1和1之间。导数为 $1-(tanh(x))^2=1-a^2$。函数图像如下图所示：

|![image](3.4.2%20tanh函数图像.jpg)|
|----|
因为`tanh`函数的值介于-1和1之间，在实际计算中，该激活函数的平均值更接近0，因此可能需要平移所有数据，让数据平均值为0。使用`tanh`函数也有类似数据中心化的效果，让下一层神经网络的学习更方便。

**缺点**：`z`特别大或特别小的时候，导数的梯度（函数的斜率）会很小，拖慢梯度下降算法。

**注意**：在大多数情况下，`tanh`函数总要优于`σ`函数。但在做二元分类时，隐藏层可以用`tanh`函数，可输出层还是要用`σ`函数。

### 3.4.3 `ReLU`函数（Rectified Linear Unit）`又称线性修正单元`

`ReLU`函数表达式为 $a=max(0,z)$ ，导数为 $(0,1)$ 。函数图像如下图：

|![image](3.4.3%20ReLU函数图像.png)|
|----|
当z为正时，导数为1。当z为负数时，导数为0。`z=0`时，导数无意义。但在编程中，`z=0`的概率很低，因此不需要担心导数无意义的情况。可以令`z=0`处的导数为0或1。

其实，`ReLU`函数有好几个变种，详见[ReLU函数](https://baike.baidu.com/item/ReLU%20%E5%87%BD%E6%95%B0)。

除了二元分类外，其他多数情况下，`ReLU`更加常用（相对于`tanh`而言）。有时候也不用普通的`ReLU`函数，而用的是`leaky ReLU`，[ReLU函数](https://baike.baidu.com/item/ReLU%20%E5%87%BD%E6%95%B0)的介绍中有这个函数。

## 3.5 神经网络需要非线性激活函数的原因

在一般情况下，对于给定的输入值`x`，有如下计算过程：

$\\ z^{[1]}=w^{[1]}x+b^{[1]}\\ a^{[1]}=g(z^{[1]})\\ z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}\\ a^{[2]}=g(z^{[2]})$

其中，`a=g(z)`为激活函数。当`g(z)=z`时，该激活函数在数学上就称为`恒等激活函数`。因为该函数直接把输入值输出了。

如果使用了上述的`恒等激活函数`，本节`3.5`开头的模型输出`y`就变成了输入特征的线性组合。具体计算如下所示：

$\\ a^{[1]}=z^{[1]}=W^{[1]}x+b^{[1]}\\ a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]} =(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})=W'x+b'$

此时只是把输入特征线性组合再输出，基本规则还是没变。

事实上，如果使用线性激活函数，或者没有激活函数，无论神经网络有多少层，做的一直是计算线性激活函数，那样做的话，相当于去掉了隐藏层。在计算房价问题上，隐藏层不能用线性激活函数，可用`ReLU`或`tanh`，此时输出层的激活函数是`g(z)=z`。
除了输出层可以使用线性激活函数外，隐藏层使用线性激活函数一般是与压缩有关的特殊情况。

## 3.6 基本激活函数的导数

当神经网络进行反向传播时，需要计算激活函数的斜率或导数。以下是几种常见的激活函数及其导数：

### 3.6.1 `sigmoid`函数

`sigmoid`函数的公式为：$g(z)=\frac{1}{1+e^{-z}}$ ，它的导数如下：

$g'(z)=\frac{d}{dz}g(z)=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=g(z)(1-g(z))$

在神经网络中，通常把`sigmoid`函数写为$a=g(z)=\frac{1}{1+e^{-z}}$，则该函数的导数就简化为`a'=a(1-a)`。使用该方法可以快速计算该函数的斜率。

### 3.6.2 `tanh`函数

`tanh`函数的公式为：$g(z)=tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$，它的导数如下：

$g'(z)=\frac{d}{dz}g(z)=1-(tanh(z))^{2}$

神经网络中使用`3.6.1`中的方法，令`a=g(z)`，则`tanh`函数的导数为：$g'(z)=1-a^{2}$

### 3.6.3 `ReLU`函数

`ReLU`函数的公式为：`g(z)=max(0,z)`，它的导数如下：

$g'(z)=\left\{\begin{matrix} 0,z<0\\ 1,z>0\\ NaN,z=0 \end{matrix}\right.$ ，`NaN表示无意义`。在程序里，可以令`z=0`时的导数为`0`或`1`。

事实上，`ReLU`函数的导数`g'(z)`是该激活函数(`ReLU`函数)的次梯度，这样的梯度下降法依然有效。

### 3.6.4 `Leaky ReLU`函数(带泄露的`ReLU`函数)

`Leaky ReLU`函数的公式为：$g(z)=max(\lambda z,z)$，其中$\lambda$的值为小于1的正数，在视频中使用的值为`0.01`。该函数的导数如下：

$g'(z)=\left\{\begin{matrix} \lambda,z<0 \\ 1,z>0\\ NaN,z=0 \end{matrix}\right.$ ，`NaN表示无意义`。在程序里，可以令`z=0`时的导数为$\lambda$或`1`。

## 3.7 神经网络的梯度下降法(单隐层)

以`3.5`的公式为例，公式如下：

$\\ z^{[1]}=w^{[1]}x+b^{[1]}\\ a^{[1]}=g(z^{[1]})\\ z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}\\ a^{[2]}=g(z^{[2]})$

在该神经网络中，有参数： $W^{[1]},b^{[1]},W^{[2]},b^{[2]},\,\,n_{x}=n^{[0]},n^{[1]},n^{[2]}$。其中， $n_{x}(n^{[0]})$ 表示有这么多的输入特征， $n^{[1]}$ 个隐藏单元， $n^{[2]}$ 个输出单元（本例只介绍 $n^{[2]}=1$ 的情况）。

此时$w{[1]}$的维度就是 $(n^{[1]},n^{[0]})$ ， $b^{[1]}$ 的维度就是 $(n^{[1]},1)$ 。 $W^{[2]}$ 的维度就是 $(n^{[2]},n^{[1]})$ ， $b^{[2]}$ 的维度就是 $(n^{[2]},1)$ 。

假定目前在做二元分类。则成本函数(`cost function`)的表达式为： $J(W^{[1]},b^{[1]},W^{[2]},b^{[2]})=\frac{1}{m}\sum_{i=1}^{n}L(\hat y,y)$ ，那么损失函数(`loss function`)跟之前的`Logistic回归`完全一样。而训练参数，就要做梯度下降。步骤如下：

```text
1.随机初始化参数。
2.计算预测值
3.计算成本函数对每个要(4声)求的参数的导数。本例中为w1、b1、w2、b2。
4.更新参数。举例：w1=w1-α*w1'，α为学习率，w1'为第三步中求得的导数。
5.重复计算步骤2、3、4。
```

在上述步骤中，关键在第三步，如何计算偏导项。具体过程还是按照之前分为正向传播和反向传播。正向传播和反向传播的公式如下所示：

$Forward\,Propagation:\\ \,\,\,\, Z^{[1]}=W^{[1]}X+b^{[1]}\\ \,\,\,\, A^{[1]}=g^{[1]}(Z^{[1]})\\ \,\,\,\, Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\ \,\,\,\, A^{[2]}=g^{[2]}(Z^{[2]})=\sigma(Z^{[2]})$

$Back\,Propagation:\\ \,\,\,\, dZ^{[2]}=A^{[2]}-Y ...... Y=[y^{(1)}\,\,y^{(2)}\,\,...y^{(m)}]\\ \,\,\,\, dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]T} \\ \\ \,\,\,\, db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},\,axis=1,\,keepdims=True) \\ \\ \,\,\,\, dZ^{[1]}=W^{[2]T}dZ^{[2]}*g^{[1]}{ }'(Z^{[1]})\\ \,\,\,\, dW^{[1]}=\frac{1}{m}dZ^{[1]}X^{T} \\ \\ \,\,\,\, db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},\,axis=1,\,keepdims=True)$

这里用到了链式法则（回顾`1.7.1`）。具体反向传播的推导过程见[视频](https://mooc.study.163.com/learn/2001281002?from=study#/learn/content?type=detail&id=2001702020)。

实现反向传播的技巧：必须保证矩阵的维度相匹配，这样可以消除反向传播的bug。

## 3.8 随机初始化参数权重

对于`Logistic回归`而言，可以将参数全部全部初始化为0。但对于一般的神经网络来说，将参数全部初始化为0再使用梯度下降法，将会完全无效。

|![image](3.8-1%20初始化参数为0示意图.png)|
|----|
以上图为例，有两个输入特征`x1、x2`，两个隐藏单元`a1、a2`，因此`n[0]=2,n[1]=2`，而与隐藏层相关的矩阵`W1`也是`(2,2)`维的。令 $W^{[1]}=\begin{bmatrix} 0 & 0\\ 0 & 0 \end{bmatrix},b^{[1]}=\begin{bmatrix} 0\\ 0 \end{bmatrix}$ （其中`b=0`进行初始化是可行的），而`W=0`就会出现问题：无论给该神经网络输入什么样本值，`a[1]_1`和`a[1]_2`的值都是一样的，因此`a[1]_1`和`a[1]_2`两个激活函数完全一样（两个隐藏单元在做完全一样的计算）。而在计算反向传播时，由于对称性，`dz[1]_1`和`dz[1]_2` 也是相同的。在假设输出权重一致的情况下，`W[2]=[0 0]`，这样`a[1]`的两个隐藏隐藏单元就完全一样（完全对称）。`dW`的每一行都是相同的，类似 $dW=\begin{bmatrix} u & v\\ u & v \end{bmatrix}$ 。经过迭代更新公式 $W^{[1]}=W^{[1]}-\alpha dW$ 的计算后，$W^{[1]}$的第一行和第二行完全一样。因此在这种情况下，多个隐藏单元无意义（因为计算的是同样的东西），当然对于更大的神经网络（或输入特征有三个、隐藏单元数目非常多）来说，也是无意义的，都在计算同样的东西，毕竟神经网络需要不同的隐藏单元计算不同的函数。

### 解决办法：随机初始化所有参数

```python
import numpy as np
W[1] = np.random.randn((2,2))*0.01      # (2,2)维的高斯分布随机变量
b[1] = np.zeros((2,1))                  # b没有对称性的问题，可以设为0。因为只要W随机初始化，依然是使用不同欧冠的隐藏单元计算不同的函数
# W[2]、b[2] 的初始化方式跟上述参数类似，W[2]的维度为(1,2)
```

为什么初始化的时候要乘以0.01呢？实际上，通常将权重矩阵初始化为非常小的随机值。在使用`sigmoid`或`tanh`激活函数时，梯度的斜率会很大（相对于这些函数比较平缓的部分），梯度下降法会很快，更新参数也会很快。
