## 神经网络和深度学习

1. [深度学习的基本步骤](1.深度学习的基本步骤(以二分类问题为例).md)
    - 1.1. sigmoid函数(作为激活函数)
    - 1.2. 损失函数（loss function）
    - 1.3. 成本函数（cost function）
    - 1.4. 梯度下降法（Gradient Descent Algorithm）
    - 1.5. 计算图（Computation Graph）
    - 1.6. 计算图的导数计算
    - 1.7. 逻辑回归中的梯度下降
2. [向量化表示](2.向量化表示.md)
    - 2.1 使用`向量化表示`的优点
    - 2.2 向量化表示进行计算的例子
    - 2.3 `Python`中的广播(boardcasting)
    - 2.4 `NumPy`的简单说明（技巧）
3. [浅层神经网络](3.浅层神经网络.md)
    - 3.1 神经网络的表示
    - 3.2 计算神经网络的输出（以Logistic回归为例）
    - 3.3 多个例子中的向量化
    - 3.4 不同的激活函数（activation function）`可以是线性的，也可以是非线性的，不同层的激活函数可以不同`
    - 3.5 神经网络需要非线性激活函数的原因
    - 3.6 基本激活函数的导数
    - 3.7 神经网络的梯度下降法(单隐层)
    - 3.8 随机初始化参数权重](3.浅层神经网络.md#38-随机初始化参数权重)
4. [深层神经网络](4.深层神经网络.md)
    - 4.1 深层神经网络符号定义
    - 4.2 深层神经网络的前向传播（以`Logistic回归`为例）
    - 4.3 深层神经网络之核对矩阵的维数
    - 4.4 使用深层表示的原因（相比浅层而言）
    - 4.5 深层神经网络块的构建
    - 4.6 前向和反向传播`Forward and Backward Propagation`
    - 4.7 参数&超参数
