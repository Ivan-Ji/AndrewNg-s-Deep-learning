<!-- TOC -->

- [2.向量化表示](#2向量化表示)
    - [2.1 使用`向量化表示`的优点](#21-使用向量化表示的优点)
        - [向量化可以消除显式的`for`循环，加速代码运行](#向量化可以消除显式的for循环加速代码运行)
            - [① 不使用向量化](#①-不使用向量化)
            - [② 使用向量化](#②-使用向量化)
    - [2.2 向量化表示进行计算的例子](#22-向量化表示进行计算的例子)
        - [2.2.1 矩阵乘法](#221-矩阵乘法)
        - [2.2.2 指数运算](#222-指数运算)
        - [2.2.3 Logistic回归(初步)](#223-logistic回归初步)
        - [2.2.4 Logistic回归(进阶)之正向传播](#224-logistic回归进阶之正向传播)
        - [2.2.5 Logistic回归(进阶)之反向传播](#225-logistic回归进阶之反向传播)
    - [2.3 `Python`中的广播(boardcasting)](#23-python中的广播boardcasting)
        - [2.3.1 对四列数据进行求和，并得到卡路里所占的百分比(不使用`for`循环)](#231-对四列数据进行求和并得到卡路里所占的百分比不使用for循环)
        - [2.3.2 向量加上数字](#232-向量加上数字)
        - [2.3.3 广播的通用规则](#233-广播的通用规则)
    - [2.4 `NumPy`的简单说明（技巧）](#24-numpy的简单说明技巧)

<!-- /TOC -->

# 2.向量化表示

## 2.1 使用`向量化表示`的优点

### 向量化可以消除显式的`for`循环，加速代码运行

在逻辑回归$z=w^{T}x+b$中，`w`和`x`都是`R`内的`nx`维的列向量。下面对`不使用向量化`和`使用向量化`进行比较：

#### ① 不使用向量化

```text
z = 0
for i in range(nx):
    z += w[i]*x[i]
z += b
```

#### ② 使用向量化

```text
import numpy as np
z = np.dot(w, x) + b
```

比较后发现，`使用向量化`比`不使用向量化`快了大约300倍。事实上，无论是`CPU`还是`GPU`都有并行化的指令`SIMD(Single Instruction Multiple Data)`，使数据并行计算。

## 2.2 向量化表示进行计算的例子

### 2.2.1 矩阵乘法

向量`u`是矩阵`A`和另一个向量`v`的乘积，不使用矩阵乘法时需要计算$u_{i}=\sum_{i=1}\sum_{j=1}A_{ij}v$，在这里需要使用两层的`for`循环，代码写起来也比较麻烦（很多行代码），但使用向量化之后只需要一行就好，如下所示：

```text
u = np.dot(A, v)
```

### 2.2.2 指数运算

非向量化表示时，代码如下：

```text
u = np.zeros((n,1))
for i in range(n):
    u[i] = math.exp(v[i])
```

使用向量化表示时，代码如下：

```text
import numpy as np
u = np.exp(v)
```

经过比较，代码非常简洁，运算速度也比较快。因此在这里推荐使用[NumPy](https://www.numpy.org.cn/)进行向量化的计算，省去计算中不需要的`for`循环

### 2.2.3 Logistic回归(初步)

不使用向量化表示的方法在上一节的[1.7.2](1.深度学习的基本步骤(以二分类问题为例).md)，而下面展示的是使用`初步向量化`的表示方法进行计算：

```text
J = 0, dw = np.zeros((nx,1)), b = 0

for i=1 to m:
    z[i] = wx[i] + b    # w已经过转置
    a[i] = σ(z[i])      # 计算预测的结果
    J += -( y[i]*log(a[i]) + (1-y[i])*log(1-a[i]) )   # 计算损失函数
    dz[i] = a[i] -y[i]      #开始计算导数
    dw += x[i]*dz[i]
    db += dz[i]
J /= m      # 全局成本函数
dw /= m
db /= m

w = w - α*dw    # 更新参数
b = b - α*db
```

### 2.2.4 Logistic回归(进阶)之正向传播

上面的向量化计算方法只是进行了初步使用向量简化了计算步骤，现在的方法将会是针对整个训练集最简便的向量化计算，不需要使用`for`循环：

```text
# X是(nx,m)的向量，整个训练集有m组数据
# Z和b是(1,m)维的向量
# 计算时使用到了矩阵的乘法，以及广播(boardcasting)，广播的方法将在本节2.3中讲解
Z = np.dot(w.T,X) + b
A = σ(Z)
```

### 2.2.5 Logistic回归(进阶)之反向传播

```text
# X是训练集的特征，Y是训练集的标签
dz = A - Y
dw = 1/m * X * dz.T
db = 1/m * np.sum(dz)
w = w - α*dw
b = b - α*db
```

## 2.3 `Python`中的广播(boardcasting)

### 2.3.1 对四列数据进行求和，并得到卡路里所占的百分比(不使用`for`循环)

```text
import numpy as np
a = [[56.0, 0.0, 4.4, 68.0],
    [1.2, 104.0, 52.0, 8.0],
    [1.8, 135.0, 99.0, 0.9]
]
A = np.array(a)
# A的横坐标分别为['apple', 'beef', 'eggs', 'potatoes']
# A的纵坐标为['carb', 'protein', 'fat']
cal = A.sum(axis=0)     # 竖直相加
# cal = array([ 59. , 239. , 155.4,  76.9])
percentage = 100 * A/cal.reshape(1,4)
```

### 2.3.2 向量加上数字

```text
import numpy as np
A = np.array([1, 2, 3, 4])
result = A + 100
# array([101, 102, 103, 104])

B = np.array([[1, 2, 3],
            [4, 5, 6]])
C = np.array([100, 200, 300])
result = B + C          # 水平复制和竖直复制数字是类似的
# array([[101, 202, 303],
#       [104, 205, 306]])
```

### 2.3.3 广播的通用规则

(m,n)与(1,n)在进行`+, -, *, /`计算时，会将(1,n)转化为(m,n)，再进行计算，(m,1)也是如此

## 2.4 `NumPy`的简单说明（技巧）

```text
import numpy as np

a = np.random.randn(5)      # 5个随机高斯变量，既不是行向量，也不是列向量，此时数组的秩为1
                            # a==a.T，如果运算np.dot(a,a.T)，得到的不是一个矩阵，而是一个数字，因此不推荐使用这个产生数组的方法

b = np.random.randn(5,1)    # (5,1)的向量，a!=a.T，运算np.dot(a,a.T)将会得到一个矩阵

assert b.shape==(5,1)       # 声明

a = a.reshape(5,1)
```
