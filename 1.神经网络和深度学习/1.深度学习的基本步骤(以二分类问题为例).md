<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- TOC -->

- [1. 深度学习的基本步骤](#1-深度学习的基本步骤)
    - [1.1. sigmoid函数(作为激活函数)](#11-sigmoid函数作为激活函数)
    - [1.2. 损失函数（loss function）](#12-损失函数loss-function)
        - [1.2.1. 损失函数为什么是这个样子呢](#121-损失函数为什么是这个样子呢)
        - [1.2.2. 取负值的原因](#122-取负值的原因)
    - [1.3. 成本函数（cost function）](#13-成本函数cost-function)
        - [1.3.1. `m`个训练样本的总体成本函数表示](#131-m个训练样本的总体成本函数表示)
        - [1.3.2. 由于训练模型时，目标是让成本函数最小化，因此不用`极大似然估计`。而去掉`1.3.1`中的负号，则需要对成本函数进行适当的缩放，因此在前面增加一个$\frac{1}{m}$](#132-由于训练模型时目标是让成本函数最小化因此不用极大似然估计而去掉131中的负号则需要对成本函数进行适当的缩放因此在前面增加一个\frac1m)
    - [1.4. 梯度下降法（Gradient Descent Algorithm）](#14-梯度下降法gradient-descent-algorithm)
    - [1.5. 计算图（Computation Graph）](#15-计算图computation-graph)
    - [1.6. 计算图的导数计算](#16-计算图的导数计算)
    - [1.7. 逻辑回归中的梯度下降](#17-逻辑回归中的梯度下降)
        - [1.7.1. 目的：计算偏导数](#171-目的计算偏导数)
        - [1.7.2. 以本节开头的逻辑回归(单样本)为例](#172-以本节开头的逻辑回归单样本为例)
        - [1.7.3. 以总体样本来看（m个样本）](#173-以总体样本来看m个样本)
            - [1.7.3.1. **方法**：运算的梯度下降法（伪代码）如下](#1731-方法运算的梯度下降法伪代码如下)
            - [1.7.3.2. 缺点**](#1732-缺点)

<!-- /TOC -->

# 1. 深度学习的基本步骤

这里以逻辑回归为例，公式为 $y=w^{T}x+b$
，其中`x`代表输入值input的特征向量，`y`代表输出值`output`，y∈{0,1}，表示输入值x的类别，1 表明 x 属于猫，0 则是不属于猫。

## 1.1. sigmoid函数(作为激活函数)

$G(z)=\frac{1}{1+e^{-z}}$

因为一元线性回归的函数图像是一条直线，而二分类问题最终需要落在`0`和`1`的结果上。当`z`越大时，$e^{-z}$ 越小，G(z) 的分母越趋近于1，因此 G(z) 越接近于1.反之，当z越小，$e^{-z}$ 越大，G(z) 的分母越趋近于无穷大，G(z)也就月接近于0。z=0.5 时，G(z)=1

## 1.2. 损失函数（loss function）

损失函数的公式表示如下所示：

$L(\hat{y},y)=-(y*log\,\hat{y}+(1-y)*log(1-\hat{y}))$
，其中激活函数为$G(z)=\frac{1}{1+e^{-z}}$

在线性回归中损失函数一般表示为 $L(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^{2}$，该函数时非凸函数，梯度下降时容易出现多个极小值。因此在二分类问题中，`Andrew Ng`推荐使用的损失函数为：

$L(\hat{y},y)=-(ylog\hat{y}+(1-y)log(1-\hat{y}))$

但损失函数指适用于单样本中。

### 1.2.1. 损失函数为什么是这个样子呢

实际上，根据计算可以得到：

```text
if y = 1:   p(y|x) = y^
if y = 0:   p(y|x) = 1 - y^
```

将上述两个式子合并之后，可以得到 $p(y|x)=\hat{y}^{y}(1-\hat{y})^{1-y}$ ，同时由于`log`函数是严格单调递增的函数，最大化`log(p(y|x))`等价于最大化`p(y|x)`，再计算`log(p(y|x))`（成本函数的负值），即

$log\,p(y|x)=log\, \hat{y}^{y}(1-\hat{y})^{1-y}=ylog\hat{y}+(1-y)log(1-\hat{y})$。

### 1.2.2. 取负值的原因

当训练学习算法时，希望算法的输出值是最大的，但在Logistic回归中，需要最小化损失函数，因此最小化损失函数就是最大化`log(p(y|x))`

## 1.3. 成本函数（cost function）

成本函数是整个训练集的损失函数的平均值，可以通过找到合适的参数w和b，使得总体的成本函数最小化。公式如下：

$J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})$

它是一个 U 型的凸函数。

### 1.3.1. `m`个训练样本的总体成本函数表示

假设所有的训练样本服从同一分布且相互独立，下面将分布讲解

1. 所有样本的联合概率，即每个样本概率的乘积为

   $p(labels\,\,in\,\,training\,\,set)=\prod_{i=1}^{m}p(y^{(i)}|x^{(i)})$
2. 对联合概率求极大似然估计。需要寻找一组参数，使得给定样本的观测值概率（联合分布概率）最大。令其最大化，等价于使其对数最大化，因此在两边取对数，可得

   $log\,p(labels\,in\,training\,set)=log\prod_{i=1}^{m}p(y^{(i)}|x^{(i)})=\sum_{i=1}^{m}log\,p(y^{(i)}|x^{(i)})$。

3. 由于在`1.2 损失函数`里提到，$log\,p(y^{(i)}|x^{(i)})=-L(\hat y^{(i)},y^{(i)})$，两者是等价的，因此可以得到

   $log\,p(labels\,in\,training\,set)=-\sum_{i=1}^{m}L(\hat y^{(i)},y^{(i)})$

   这里就推导出了前面的成本函数。

### 1.3.2. 由于训练模型时，目标是让成本函数最小化，因此不用`极大似然估计`。而去掉`1.3.1`中的负号，则需要对成本函数进行适当的缩放，因此在前面增加一个$\frac{1}{m}$

## 1.4. 梯度下降法（Gradient Descent Algorithm）

**目的**：训练参数，在二分类问题中为了训练并更新`w`和`b`。

函数`J(w,b)`关于`w`和`b`的方程，J(w,b)总是沿着下降最快的方向减小，并不断迭代，实时更新。更新参数的公式如下所示：

$w'=w-\alpha \frac{\partial J(w,b)}{\partial w}$

$b'=b-\alpha \tfrac{\partial J(w,b)}{\partial b}$

其中，$\frac{\partial J(w,b)}{\partial w}$ 表示函数`J(w,b)`关于自变量`w`的斜率，$\tfrac{\partial J(w,b)}{\partial b}$ 表示函数`J(w,b)`关于自变量`b`的斜率，`α`表示学习率，学习率的选取将在后面的章节讨论

## 1.5. 计算图（Computation Graph）

在例子`J(a,b,c)=3(a+bc)`中，分步骤进行计算，令`u=bc, v=a+u, J=3v`，然后按照节点表示这个计算过程。

## 1.6. 计算图的导数计算

借用 1.5 的例子，可以得到公式：$\frac{\partial J}{\partial v}=3$

通过计算该式子，可以方便计算下面式子的值

$\frac{\partial J}{\partial a}=3=\frac{\partial J}{\partial v}*\frac{\partial v}{\partial a}$

$\frac{dJ}{du}=3=\frac{dJ}{dv}*\frac{dv}{du}$

这里用到了导数的链式法则。求导是为了反向传播的计算，在程序里用`dvar`表示变量`var`的导数。

## 1.7. 逻辑回归中的梯度下降

### 1.7.1. 目的：计算偏导数

### 1.7.2. 以本节开头的逻辑回归(单样本)为例

假定`w`有两个参数`w1`，`w2`,`x`有两个特征值`x1`,`x2`,外加参数`b`,于是可以得到公式:

1. 回归公式   $z=w_{1}x_{1}+w_{2}x_{2}+b$
2. sigmoid函数 $a=\sigma (z)$
3. 损失函数 $L(a,y)$

- 首先可以求得
  
  $da=\frac{\partial L(a,y)}{\partial a}=-\frac{y}{a}+\frac{1-y}{1-a}$
- 其次可以通过链式法则求得
  $dz=\frac{\partial L}{\partial z}=\frac{\partial L(a,y)}{\partial z}=\frac{\partial L}{\partial a}*\frac{\partial a}{\partial z}=(-\frac{y}{a}+\frac{1-y}{1-a})*a(1-a)=a-y$
- 同理可得 $dw_{1}=\frac{\partial L}{\partial w_{1}}=x_{1}dz,\, dw_{2}=\frac{\partial L}{\partial w_{2}}=x_{2}dz,\, db=dz$。

计算完导数之后，就可以使用`1.4 梯度下降法`中的公式对参数进行更新。

### 1.7.3. 以总体样本来看（m个样本）

- 成本函数的公式为
  
  $J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(a^{(i)},y^{(i)})$
  
  它是对所有样本求损失函数值,其中，$a^{(i)}$ 是样本的预测值，$y$是样本的真实值。
- $a^{(i)}$ 的具体计算公式如下所示：
  
  $a^{(i)}=\hat y^{(i)}=\sigma (z^{(i)})=\sigma (w^{T}x^{(i)}+b)$

- 计算结束之后对各个参数求导，可以求得 $dw_1^{(i)},dw_2^{(i)},db^{(i)}$

- 求得全局成本函数（第1项到第m项 损失函数和的平均）
  
  $\frac{\partial }{\partial w_{1}}J(w,b)=\frac{1}{m}\sum_{i=1}^{m}\frac{\partial}{\partial w_{1}}L(a^{(i)},y^{(i)})$

#### 1.7.3.1. **方法**：运算的梯度下降法（伪代码）如下

1. 初始化各个参数，令 $J=0;dw_{1}=0;dw_{2}=0;db=0$
2. 使用`for`循环进行计算，`w`是指特征参数，可能有多个:

```text
for i=1 to m:
    z[i] = w*x[i] + b   # 这里的w进行了转置
    a[i] = σ(z[i])      # 计算预测的结果
    J += -( y[i]*log(a[i]) + (1-y[i])*log(1-a[i]) )   # 计算损失函数
    dz[i] = a[i] -y[i]      #开始计算导数
    dw1 += x1[i]*dz[i]
    dw2 += x2[i]*dz[i]
    db += dz[i]
J /= m      # 全局成本函数
dw1 /= m
dw2 /= m
db /= m

w1 = w1 - α*dw1    # 更新参数
w2 = w2 - α*dw2
b = b - α*db
```

3.重复步骤`2`，次数为`n`。

#### 1.7.3.2. 缺点

使用`for`循环进行计算时，运算速度慢，效率低下。而且深度学习要处理的数据量十分庞大，因此需要进行优化。优化方法具体看下节 [2.向量化表示](2.向量化表示.md)
