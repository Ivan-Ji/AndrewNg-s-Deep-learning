<!-- TOC -->

- [2. 自然语言处理与词嵌入](#2-%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%e4%b8%8e%e8%af%8d%e5%b5%8c%e5%85%a5)
  - [2.1 词汇表征](#21-%e8%af%8d%e6%b1%87%e8%a1%a8%e5%be%81)
  - [2.2 使用词嵌入](#22-%e4%bd%bf%e7%94%a8%e8%af%8d%e5%b5%8c%e5%85%a5)
    - [基于 word embedding 的迁移学习](#%e5%9f%ba%e4%ba%8e-word-embedding-%e7%9a%84%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0)
    - [词嵌入 word embedding 和人脸编码 face encoding](#%e8%af%8d%e5%b5%8c%e5%85%a5-word-embedding-%e5%92%8c%e4%ba%ba%e8%84%b8%e7%bc%96%e7%a0%81-face-encoding)
  - [2.3 Word Embedding 的特性](#23-word-embedding-%e7%9a%84%e7%89%b9%e6%80%a7)
    - [最常用的相似度函数](#%e6%9c%80%e5%b8%b8%e7%94%a8%e7%9a%84%e7%9b%b8%e4%bc%bc%e5%ba%a6%e5%87%bd%e6%95%b0)
  - [2.4 Embedding 矩阵](#24-embedding-%e7%9f%a9%e9%98%b5)
  - [2.5 学习词嵌入`Word Embedding`](#25-%e5%ad%a6%e4%b9%a0%e8%af%8d%e5%b5%8c%e5%85%a5word-embedding)
  - [2.6 word2vec](#26-word2vec)
    - [word2vec 模型之 Skip-grams（用中间词预测两边的词）](#word2vec-%e6%a8%a1%e5%9e%8b%e4%b9%8b-skip-grams%e7%94%a8%e4%b8%ad%e9%97%b4%e8%af%8d%e9%a2%84%e6%b5%8b%e4%b8%a4%e8%be%b9%e7%9a%84%e8%af%8d)
    - [模型流程](#%e6%a8%a1%e5%9e%8b%e6%b5%81%e7%a8%8b)
    - [存在的问题](#%e5%ad%98%e5%9c%a8%e7%9a%84%e9%97%ae%e9%a2%98)
      - [简化方案：`Hierarchical Softmax` 分级的softmax（自己查公式）](#%e7%ae%80%e5%8c%96%e6%96%b9%e6%a1%88hierarchical-softmax-%e5%88%86%e7%ba%a7%e7%9a%84softmax%e8%87%aa%e5%b7%b1%e6%9f%a5%e5%85%ac%e5%bc%8f)
    - [如何采样上下文`(C, content)`](#%e5%a6%82%e4%bd%95%e9%87%87%e6%a0%b7%e4%b8%8a%e4%b8%8b%e6%96%87c-content)
  - [2.7 负采样 Negative Sampling](#27-%e8%b4%9f%e9%87%87%e6%a0%b7-negative-sampling)
    - [负采样的计算过程](#%e8%b4%9f%e9%87%87%e6%a0%b7%e7%9a%84%e8%ae%a1%e7%ae%97%e8%bf%87%e7%a8%8b)
    - [k 的选取](#k-%e7%9a%84%e9%80%89%e5%8f%96)
    - [预测方法](#%e9%a2%84%e6%b5%8b%e6%96%b9%e6%b3%95)
    - [如何选取负样本](#%e5%a6%82%e4%bd%95%e9%80%89%e5%8f%96%e8%b4%9f%e6%a0%b7%e6%9c%ac)
  - [2.8 GloVe 词向量（Global Vector for Word representation）](#28-glove-%e8%af%8d%e5%90%91%e9%87%8fglobal-vector-for-word-representation)
    - [GloVe 计算步骤](#glove-%e8%ae%a1%e7%ae%97%e6%ad%a5%e9%aa%a4)
  - [2.9 情感分类](#29-%e6%83%85%e6%84%9f%e5%88%86%e7%b1%bb)
    - [2.9.1 平均值或和预测](#291-%e5%b9%b3%e5%9d%87%e5%80%bc%e6%88%96%e5%92%8c%e9%a2%84%e6%b5%8b)
    - [2.9.2 RNN 模型](#292-rnn-%e6%a8%a1%e5%9e%8b)
  - [2.10 词嵌入消除偏见](#210-%e8%af%8d%e5%b5%8c%e5%85%a5%e6%b6%88%e9%99%a4%e5%81%8f%e8%a7%81)
    - [机器学习中的偏见](#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%81%8f%e8%a7%81)
    - [消除偏见的方法](#%e6%b6%88%e9%99%a4%e5%81%8f%e8%a7%81%e7%9a%84%e6%96%b9%e6%b3%95)

<!-- /TOC -->

# 2. 自然语言处理与词嵌入

## 2.1 词汇表征

词嵌入(word embedding)是语言表示的一种方式，可以让算法自动理解一些含义相近的词。通过词嵌入的概念，即使模型标记的训练集相对较小，也可以构建NLP应用。

之前我们将对词的表示用的是one-shot，这种方法简单，这种表示方法的一个缺点就是，它把每个词都孤立起来了，使得其对相关词的泛化能力不强，因为任意两个one-shot向量内积都为0 ，也就是任意两个词间的距离是一样的，无法表示词与词之间的相关性。

如果我们不用one-hot表示而是用特征化表示(featurized representation)来表示每个词，对于不同的单词算法会泛化的更好。这种高维特征的表示能够比one-hot更好的表示不同的单词。如下图所示，比如可以用`gender, royal, age, food`等300个不同的特征来表示一个词。而向量中的数字就表示词在该特征下的相关度。例如，单词`man`使用向量 $e_{5391}$ 表示，而该向量的维度为300维，此时的`5391`则代表该单词是词典中的第`5391`个词。

|![image](2.1-1%20词汇表征之词的特征化表示.png) |
|----|

最终，学习的特征（300维的词嵌入）通常可以把这300维的数据嵌入到一个二维空间里可视化，常用的数据降维算法是`t-SNE算法`（使用非线性的方式），从下图中可以看出，相似的词距离比较近，不同类的词距离比较远。。

|![image](2.1-2%20词汇表征之二维空间可视化.png) |
|----|

## 2.2 使用词嵌入

`Sally Johnson is an orange farmer.`

以人名实体识别为例，上述句子的`Sally Johnson`应该是一个人名而不是其他的，因为`farmer`必须为一个人。如果用一个基于词嵌入训练的模型，对于另外一句话`Robert Lin is an apple farmer`，因为知道橘子和苹果属于一类，那么就很容易推断出`Robert Lin`也是一个人名。如果遇到很少见的词如`durian cultivator`，假如训练集里面没有这两个词，那么预测就会很困难，但是如果有学习好的 word embedding，预测就会很简单。

词嵌入能达到这样的效果，是因为学习 word embedding 的算法通常需要大量的无标签文本库，来源可以是互联网文本或可以免费下载的文本。可以利用迁移学习使用 word embedding 。

### 基于 word embedding 的迁移学习

1. 从一个很大的文本中学习 word embedding ，或从网上下载学习好的 word embedding ；
2. 将学习好的 embedding 应用到只有少量带标签的训练集的新任务中；
3. 可选项：决定是否根据新任务的训练集对 word embedding 进行微调，当新任务的训练集较小的时候，不推荐花费精力调整 word embedding 。

迁移学习对于训练集较小的任务比较有意义，所以目前词嵌入广泛应用于NLP，如实体识别、文本摘要、文本解析等标准的NLP任务。因为任务中有大量的数据，在语言模型、机器翻译等领域应用较少。

### 词嵌入 word embedding 和人脸编码 face encoding

word embedding 学习的是词的编码，而 face encoding 学习的是人脸或图像的编码，术语 embedding 和 encoding 是可以互换的。它们的区别如下：

* face encoding 学习的“编码/特征”没有范围限定，也就是预先不需要设置编码，所以任何输入都能学习编码；
* word embedding 则有一个固定的词汇表，所以学习的编码也是固定的。

## 2.3 Word Embedding 的特性

词嵌入`Word Embedding`可以实现类比推理。通过不同词向量之间的相减计算，可以发现不同词之间的类比关系。

|![image](2.3-1%20Word%20Embedding的特性之词的特征.png) |
|----|

上图是用词嵌入学习特征表示的几个词，`man`如果对应`woman`，那么`king`对应哪个词？如何让算法学习到这种推理关系？

1. 用编码向量表示各个词 $e_{man}, e_{woman}, e_{king}, e_{queen}...$ ，$man(5391)$中 5391 标示的是 $man$ 这个词在词汇表中的位置，该向量的特征编码如下：

    * $e_{man}=[−1,0.01,0.03,0.09]^T$

2. 通过编码向量找到一个词 $w$ 使得 $e_{king}−e_{man}≈e_{woman}-e_w$ 成立，即找到与 $e_{king}−e_{man}+e_{woman}$ 相似性最高的词，也就是所找词 $e_w$ 计算方法如下：

    * $max:\,\,\,sim(e_w,e_{king}−e_{man}+e_{woman})$
3. 计算编码差发现，`man`和`woman`的主要差异在第一个参数也就是性别，而`King`和`queen`的主要差异也是性别。如下所示：

    * $e_{man}−e_{woman}≈[−2,0,0,0]^T$
    * $e_{king}−e_{queen}≈[−2,0,0,0]^T$

### 最常用的相似度函数

* 余弦函数 $sim(u,v)=cos\theta =\frac{u^Tv}{||u||_2||v||_2}$
* 平方距离 $sim(u,v)=||u−v||^2$

## 2.4 Embedding 矩阵

应用算法学习词嵌入实际学习的是 Embedding 矩阵。

如果词汇表长度是10000，加上`UNK`就是10001，特征为300个，那么我们要得到的矩阵 E 的维度就是(300,10001)。矩阵每一列代表词汇表中的一个词，是通过词在词汇表的位置来表示的，即 `one-hot` 向量里面 1 的位置，用 $O$ 来表示每个词的 `one-hot` 向量，那么词的`Embedding`表示（特征编码）的计算公式为：

* $e_j=E∗O_j$

其中：

* j ：代表词在词汇表中的位置，如 orange 是6257
* e ：表示词嵌入向量
* E ：嵌入矩阵，包含所有词汇和所有特征，横轴为词汇表，纵轴为词汇的特征
* O ：词的`one-hot`向量

实际中并不利用矩阵乘法来计算嵌入向量，因为词汇表通常很大，`one-hot`向量也会很大，但是里面只有一个有用值，其他都为 0 ，所以矩阵相乘大部分都是在做 0 乘法运算。实际中一般都直接从`Embedding`矩阵提取单列的`Embedding`向量，如在`Keras`中有个`embedding layer`可以高效的从嵌入矩阵中提取`Embedding`向量，而不是进行慢且复杂的矩阵乘法运算。

## 2.5 学习词嵌入`Word Embedding`

早期的词嵌入算法比较复杂，随着时间发展，现在（大数据时代）变得越来越简单

1. 训练语言模型时通常选择目标词的前几个词来预测目标词。(表示词的时候使用次的索引，历史窗口常选4)

    每个单词获得one-hot向量后，乘以嵌入矩阵E得到嵌入向量e（假设为300维），然后将所有嵌入向量作为神经网络输入，经过神经网络层之后，通过softmax层输出预测，softmax分类器输出数目与词汇表数目一致，通过输出的各词出现的概率，预测此位置最可能的词。最后利用反向传播学习各个参数以及 Embedding 矩阵`E`。如下图所示：

    |![image](2.5-1%20训练词嵌入之嵌入矩阵的学习.png) |
    |----|

    实际中也可以设置一个超参数来决定用几个词来进行预测，比如 4 个，用固定数目的词组成的隐藏层叫做 **历史窗口**，因为其维度固定不受句长的影响，所以可以处理任意长度的句子。
2. 在其他的词嵌入中可以利用不同的方法选取上下文：
   * 选取目标词之前的几个词（用于构建语言模型，下面的三个方法用于学习词嵌入）
   * 选取目标词前后的几个词（CBOW 连续词袋模型的思想）
   * 选取目标词前的一个词
   * 选取目标词附近的一个词（Skip-Gram 模型的思想）

## 2.6 word2vec

Word2Vec算法是一种简单的计算更加高效的方式来实现对词嵌入的学习。包含 CBOW 连续词袋模型和 Skip-gram 模型，本节只介绍后者。

### word2vec 模型之 Skip-grams（用中间词预测两边的词）

抽取上下文（Content）和目标词（Target）配对，来构造一个监督学习问题。

随机选择一个词作为上下文，同时在上下文的一定距离范围内（即词距）随机选择另外一个词作为目标词。

### 模型流程

* 词汇表：`vocab_size = 10000`
* 基本监督学习：构建上下文`(C, content)`和目标词`(T, target)`的映射关系：C —> T
* $O_c$（one-hot）—> E（词嵌入矩阵）—> $e_c=E∗O_c$（词嵌入）—> Softmax 层 —> 输出 $ŷ$
* softmax： $\theta$ 是与输出 t 有关的参数，公式如下：

    $p(t|c)=\frac{e^{θ_t^T e_c}}{\sum_{j=1}^{10000}e^{θ_j^T e_c}}$
* 损失函数：softmax 经常使用的损失函数，$\hat{y}$ 此时是 `one-hot` 向量，$y$向量表示所有可能目标词的概率

    $L(\hat{y},y)=−∑_{i=1}^{10000}y_ilog\hat{y}_i$

* 反向传播梯度下降进行训练，得到模型的参数 E 和 softmax 的参数

### 存在的问题

在Softmax单元中，需要对所有10000个整个词汇表的词做求和计算（求和计算发生在 softmax 层分母上），计算量庞大。

#### 简化方案：`Hierarchical Softmax` 分级的softmax（自己查公式）

使用`Hierarchical Softmax`分类器（ **折半查找**，相当于一个二叉树分类器，每个节点处将所有词汇分成两份，使用 sigmoid 来确定目标词是在前半部分还是后半部分，然后一级一级的分下去，每个节点是一个 logistic 分类器，不需要为单次分类对词汇表中的词进行求和，直到找到这个词），其计算复杂度是前面的 log|v| 级别（ v 是词汇表大小）。在构造分级softmax分类器时，一般常用的词会放在树的顶部位置，而不常用的词则会放在树的更深处。并不是一个平衡的二叉树。

### 如何采样上下文`(C, content)`

在构建上下文目标词对时，如何选择上下文与模型有不同的影响。

* 对语料库均匀且随机地采样：使得如the、of、a等这样的词会出现的相当频繁，导致上下文和目标词对经常出现这类词汇，但我们想要的目标词却很少出现。这样需要花更长的时间来训练更新参数
* 采用不同的启发来平衡常见和不常见的词进行采样。(实际使用的方法)。

## 2.7 负采样 Negative Sampling

Skip-gram 随机选取目标词，构建监督学习任务，能够学习到一个实用的词嵌入模型。但是softmax 计算的时间复杂度较高。

有一种一种改善的学习问题：负采样。其能够做到的和Skip-grams模型相似，但其学习算法更加有效。简单来说就是选一组词，然后判断其是否有关联。

### 负采样的计算过程

`I want a glass of orange juice to go along with my cereal`

在这个算法中要做的是构造一个新的监督学习问题，以上述句子为例，问题就是给定一对单词，比如 orange, juice，去预测这是否是一对上下文词（即目标词）。生成这些数据的方式是选择一个上下文词再选一个目标词，作为正样本，记为 1 ；然后给定几次用相同的上下文再从字典中选取随机的词，作为负样本`negative example`，记为 0 。例如，orange 和 juice 是一对正样本，orange 和 king 是一对负样本。选定上下文后，选取一个目标词标记为 1 （与上下文词有关系），选取 k 个非目标词(负采样)标记为 0 （与上下文词无关），建立模型进行训练。

|![image](2.7-1%20负采样之一对词的监督学习.png) |
|----|

接下来构造一个监督学习问题，其中输入 x 是一对词（包含上下文词和目标词），输出为预测目标的标签即预测输出 y（0 或 1），因此问题就是给定一对词，像 orange 和 juice ， 这个算法就是要分辨两种不同的采样方式。这就是如何生成训练集的方法。

### k 的选取

小数据集，k=5～20 ；大数据集，k=2～5（数据集越大，k越小）

### 预测方法

* 将庞大维度的 softmax 改为 k+1 个二分类问题，公式如下：

    $P(y=1|c,t)=σ(θ^T_te_c)$

|![image](2.7-2%20负采样之预测流程.png) |
|----|

**预测流程**：输入 context 的 one-hot 向量，传递给嵌入向量 E ，然后两者相乘得到嵌入向量 e，输出有10000（词汇表长度）种二分类可能，也就是由 10000 个二分类 logistics 回归分类器，根据与 context 配对的词来确定是哪个分类器，每次计算更新 k+1 个二分类结果，计算量大大减少。所以softmax不使用 10000 维的数据之和，而是把数据变为 10000 维的二分类问题。

### 如何选取负样本

* 单词频率：导致一些类似`a, the, of`等词的频率较高
* 均匀随机：对英文文本没有很好的代表性
* 基于上述两个极端，针对英文文本抽取某个词的概率推荐如下：

    $$P(w_i)=\frac{f(w_i)^{3/4}}{∑^{10000}_{i=1}f(w_i)^{3/4}}$$

    其中 $f(w_i)$ 为观测到的词 i 在语料库中出现的频率，通过这种方式使得分布在两个极端之间。

现在有很多开源资源，如果想要在NLP领域取得快速进展，可以下载词向量，并在此基础上改进。

## 2.8 GloVe 词向量（Global Vector for Word representation）

GloVe 词向量模型是另一种计算词嵌入的方法，虽然没有 Skip-grams 模型用的多，但这种模型更加简单。

### GloVe 计算步骤

GloVe 词向量模型要做的是使得“上下文词”与“目标词”的关系明确化。在该模型中，要定义一个量 $X_{ij}$，表示单词 $i$ 出现单词 $j$ 上下文中的次数（共现次数：将“上下文词”与“目标词”的范围为左右各 10个词以内时 $X_{ij}=X_{ji}$ ；如果对“上下文词”的选择为“上下文词”总是在“目标词”前一个，$X_{ij}$ 、$X_{ji}$ 就不会对称）。模型的优化目标为最小化如下公式：

$$∑_{i=1}^{10000}∑_{j=1}^{10000}f(X_{ij})(θ^T_ie_j+b_i+b'_j−logX_{ij})^2$$

* 其中，因为当 $X_{ij}$ 为 0 时，$logX_{ij}$ 便没有意义，所以添加 $f(X_{ij})$ 的加权项，当 $X_{ij}=0$ 时，$f(X_{ij})=0$。另外 $f(X_{ij})$ 对于一些频繁词和不频繁词有着启发式的平衡作用（看GloVe算法论文）
* 另外，$\theta_i^Te_j$ 这一项中，$\theta_i^T$ 和 $e_j$ 都是需要学习的参数，在这个目标算法中二者是对称的，所以我们可以一致地初始化 $\theta$ 和 $e$，然后用梯度下降来最小化输出，在处理完所有词后，直接取二者的平均值作为词嵌入向量：$e^{final}_w=(e_w+\theta_w)/2$，这与前面的算法有所不同。

输出能够对上下文和目标两个词同时出现的频率进行很好的预测，从而得到我们想要的词嵌入向量。

使用词嵌入算法（例如 GloVe 算法）学习 embedding 时，不能保证 embedding 向量的独立组成部分是能够理解的。

## 2.9 情感分类

情绪分类就是通过一段文本来判断喜好态度，这是NLP中最重要的模块之一。如下是进行情感分类的两种方法：

### 2.9.1 平均值或和预测

1. 获取一个训练好的词嵌入矩阵 E ；
2. 得到每个词的词嵌入向量（使用`one-hot`向量乘以嵌入矩阵 E ），并对所有的词向量做平均或者求和；
3. 输入到softmax分类器中，得到最后的输出 $ŷ$ 。

缺点：没有考虑词序，比如`not so good`看到`good`以为是正向情感。

### 2.9.2 RNN 模型

1. 获取一个训练好的词嵌入矩阵 E ；
2. 得到每个词的词嵌入向量（使用`one-hot`向量乘以嵌入矩阵 E ），输入到`many-to-one`的`RNN`模型中；
3. 通过最后的 softmax 分类器，得到最后的输出 $ŷ$ 。

优点：考虑了词序，效果好很多。

## 2.10 词嵌入消除偏见

人工智能算法已经被应用到做一些非常重要的社会决策中，因此我们需要尽可能地保证其不受非预期形式的偏见的影响，如性别、种族歧视等等。

### 机器学习中的偏见

机器学习的偏见来自于语料库中的偏见，比如学习的文章是有偏见的作者写的。

### 消除偏见的方法

1. 定义偏见的方向：如性别
   * 对大量性别相对的词汇进行相减并求平均：$e_{he}−e_{she}、e_{male}−e_{female}$ 。也可以用更复杂的算法除偏，例如 SVU（singular value decomposition，奇异值分解）、PC（principle component analysis，主成分分析）
   * 通过平均后的向量，则可以得到一个或多个偏见趋势相关的维度，以及大量不相关的维度
2. 中和化
   * 对性别定义不明确的词汇投影到无偏见相关维度的距离中轴线上，如 docter，nurse
3. 均衡化
   * 将性别定义明确的词调整到距离无偏见相关维度的中轴线等距的一对点上，如grandmother和grandfather

|![image](2.10-1%20词嵌入消除偏见之除偏方法示意图.png) |
|----|
