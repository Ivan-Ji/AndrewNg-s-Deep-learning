<!-- TOC -->

- [1. 循环序列模型](#1-循环序列模型)
    - [1.1 数学符号定义](#11-数学符号定义)
    - [1.2 One-hot 表示法](#12-one-hot-表示法)
    - [1.3 循环神经网络（RNN）](#13-循环神经网络rnn)
        - [1.3.1 为什么不用标准的神经网络](#131-为什么不用标准的神经网络)
        - [1.3.2 循环神经网络（RNN）的基本概念](#132-循环神经网络rnn的基本概念)
        - [1.3.3 RNN 详细介绍之公式](#133-rnn-详细介绍之公式)
        - [1.3.4 RNN 详细介绍之参数](#134-rnn-详细介绍之参数)
        - [1.3.5 RNN 详细介绍之公式简化](#135-rnn-详细介绍之公式简化)
            - [简化思路](#简化思路)
        - [1.3.6 RNN 的缺点](#136-rnn-的缺点)
    - [1.4 通过时间的反向传播（Backpropagation through time）](#14-通过时间的反向传播backpropagation-through-time)
    - [1.5 不同类型的 RNN](#15-不同类型的-rnn)
        - [1.5.1 `many-to-many`多对多类型](#151-many-to-many多对多类型)
        - [1.5.2 `many-to-one`多对一类型](#152-many-to-one多对一类型)
        - [1.5.3 `one-to-many`一对多类型](#153-one-to-many一对多类型)
    - [1.6 语言模型和序列生成](#16-语言模型和序列生成)
        - [1.6.1 什么是语言模型呢](#161-什么是语言模型呢)
        - [1.6.2 如何建立一个语言模型](#162-如何建立一个语言模型)
    - [1.7 新序列采样](#17-新序列采样)
    - [1.8 RNN 的梯度消失](#18-rnn-的梯度消失)
    - [1.9 GRU 单元（Gated Recurrent Unit, 门控循环单元）](#19-gru-单元gated-recurrent-unit-门控循环单元)
        - [RNN 回顾](#rnn-回顾)
        - [简化的 GRU 单元](#简化的-gru-单元)
        - [完整的 GRU 单元](#完整的-gru-单元)
        - [总结](#总结)
    - [1.10 LSTM （long short-term memory, 长短期记忆）](#110-lstm-long-short-term-memory-长短期记忆)
    - [1.11 双向神经网络](#111-双向神经网络)
    - [1.12 深层循环神经网络](#112-深层循环神经网络)

<!-- /TOC -->

# 1. 循环序列模型

常见的**序列模型**（Sequence Model）使用场景如下：

* 在进行语音识别时，给定了一个输入音频片段X，并要求输出片段对应的文字记录Y，这个例子里的输入和输出数据，都是序列数据。

* 序列模型在处理DNA序列分析中也十分有用，你的DNA可以用`A C G T`四个字母来表示，所以给定一段DNA序列，你能够标记处哪部分是匹配某种蛋白质。

* 在机器翻译过程中，输入一句话，需要翻译成另外一种语言。

* 在进行视频行为识别时，你可能会得到一系列视频帧，然后要求你识别其中的行为。

* 在进行命名实体识别时，可能会给定一个句子，要你识别出句中的人名。

除此之外，还有音乐生成、情感分类、文本生成等，都用到了序列模型。所有这些问题都可以被称作使用标签数据 $(X,Y)$ 作为训练集的监督学习。但是，序列模型有很多种不同类型：1. 输入数据 $X$ 和输出数据 $Y$ 都是序列；2. 输入数据 $X$ 是序列，而输出数据 $Y$ 不是；3. 输出数据 $Y$ 是序列，而输入数据 $X$ 不是。而且第一种情况下的 $X$ 和 $Y$ 有时也会不一样长。

## 1.1 数学符号定义

假设一个序列模型的输入语句 $X$ 为：`Harry Potter and Hermione Granger invented a new spell`

输入是9个单词组成的序列，所以最终我们会有9个特征集合来表示这9个单词，并按序列中的位置进行索引。这里用 $X^{<1>},X^{<2>},X^{<3>}...... X^{<9>}$ 来索引不同的位置。用 $t$ 表示该序列的位置， $X^{\left \langle t \right \rangle}$ 表示 $t$ 位置所代表的词。

输出数据也是一样，用 $Y^{<1>},Y^{<2>},Y^{<3>}...... Y^{<9>}$ 表示输出数据，同时用 $T_x$ 表示输入序列的长度，用 $T_y$ 表示输出序列的长度。这个句子中输入是9个单词，所以  $T_x=9, T_x=T_y=9$ 。

基于之前的定义，本节使用  $X^{(i)\left \langle t \right \rangle}$ 表示训练样本 $i$ 的输入序列中的第 $t$ 个元素，如果 $T_x$ 是序列的长度，那么训练集里不同的训练样本就会有不同的长度，所以 $T_x^{(i)}$ 就表示第 $i$个训练样本的输入序列长度。同样，$Y^{(i)\left \langle t \right \rangle}$ 代表第 $i$ 个训练样本输出序列中第 $t$ 个元素，$T_y^{(i)}$ 就是第 $i$ 个训练样本的输出序列的长度。所以，本节开头的输入语句 $T_x^{(i)}=9$ 。

## 1.2 One-hot 表示法

表示一个句子里的单词的方法就是做一张**词表**，有时也称为**词典**。意思是列出所有要用到的单词。例如词表中的第一个词是`a`，第二个词是`aaron`，然后更下面的是单词`and`，再后面是`harry`，直到最后，词典最后一个单词可能是`zulu`。

接下来就可以用one-hot表示法来表示词典里的每个单词，举个例子，$x^{<1>}$表示Harry这个单词，它是一个第4075行是1，其余值都是0的向量，因为4075是Harry在这个词典里的位置。而如果词典大小是10000的话，那么每个向量都是10000维的。

所以这种表示方法中，$x^{\left \langle t \right \rangle}$ 指代句子里的任意词，它就是一个one-hot变量，因为这个向量中只有一个值是1，其余值都是0。目的是用这样的表示方法表示X，用序列模型在X和目标输出Y之间学习建立一个映射

如果遇到了一个不在你词表中的词，那么就创建一个新的标记，也就是一个叫做unknown word的单词（UNK，未登录词），用这个单词作为标记来表示不在词表中的单词。

## 1.3 循环神经网络（RNN）

### 1.3.1 为什么不用标准的神经网络

使用标准的神经网络会出现下列两个问题：

1. **输入和输出数据在不同的例子中可以有不同的长度**。所有的例子不一定有着同样的输入长度 $T_x$ 或者同样的输出长度 $T_y$ ，即使每个句子都有最大长度，也许你能够填充(pad)或者零填充，使每个输入语句都能达到最大长度。但看起来仍然不是一个好的表达方式。
2. **一个单纯的神经网络结构，它并不共享从文本的不同位置上学到的特征**。具体来说，如果神经网络已经学习到了在位置1出现的harry可能是人名的一部分，那么如果harry出现在其它位置 $x^{\left \langle t \right \rangle}$ 时，它也能够自动识别其为人名的一部分的话，这就很棒了。这可能类似于你在卷积神经网络中看到的，你希望将部分图片里学习到的内容快速推广到图片的其它部分，而我们希望对序列数据也有相似的效果。

因此，这里选择循环神经网络，不仅可以解决上述两个问题，还可以减少模型中的参数数量

### 1.3.2 循环神经网络（RNN）的基本概念

`Harry Potter and Hermione Granger invented a new spell`

如果以从左到右读上述句子，第一个单词表示为 $x^{<1>}$ ，我们要做的就是将第一个词输入神经网络层，然后让神经网络预测输出。循环神经网络做的是，当它读到句中的第二个单词时，假设是 $x^{<2>}$ ，它不是仅用 $x^{<2>}$ 就预测出 $\hat{y}^{<2>}$ ，它也会输入一些来自第一个单词的信息。

具体而言，第一个单词（第一步）的激活值传递到第二个单词（第二步）。然后在下一步，循环神经网络输入单词 $x^{<3>}$ ，并把第二个单词的激活值传递到第三个单词，然后进行预测，输出预测结果 $\hat{y}^{<3>}$ ，以此类推，一直到最后一个时间步，输入 $x^{<T_x>}$ ，然后输出了 $\hat{y}^{<T_y>}$ 。如果 $T_x$ 和 $T_y$ 不相同，这个结构需要做出一些改变。所以在每一步计算中，循环神经网络传递一个激活值到下一个时间步中用于计算。

|![image](1.3-1%20循环神经网络之RNN结构图.png) |
|----|

在一些研究论文或书中，RNN可以表示为：在每一步中，有输入 $x^{\left \langle t \right \rangle}$ 和输出 $\hat{y}^{\left \langle t \right \rangle}$ 。为了表示循环连接，在隐藏层中画一个圈，表示输回网络层，有时会画一个黑色方块表示在这个黑色方块处会延迟一步。

### 1.3.3 RNN 详细介绍之公式

* $a^{\left \langle t \right \rangle}=g(W_{aa}a^{\left \langle t-1 \right \rangle}+W_{ax}x^{\left \langle t \right \rangle}+b_a)$
* $\hat{y}^{\left \langle t \right \rangle}=g(W_{ya}a^{\left \langle t \right \rangle}+b_{y})$

### 1.3.4 RNN 详细介绍之参数

1. 在零时刻需要构造一个激活值 $a^{<0>}$ ，通常是零向量，也可以用其它方法随机初始化。然后把它作为初始参数传入模型中。
2. $W_{ax}$ 表示从 $X^{<1>}$ 到隐藏层的一系列参数，激活值是由参数 $W_{aa}$ 决定，输出结果由参数 $W_{ya}$ 决定。每一步使用的都是相同的参数 $W_{ax},W_{aa}$ 。

$W_{ax}$ 中 $x$ 意味着 $W_{ax}$ 要乘以某个 $x$ 的量，第一个 $a$ 的意思是它是用来计算某个 $a$ 的量。

$W_{aa}$ 中 $x$ 意味着 $W_{ax}$ 要乘以某个 $a$ 的量。

$W_{ax}$ 中 $x$ 意味着 $W_{ax}$ 要乘以某个 $a$ 的量，$y$ 的意思是它是用来计算某个 $y$ 的量。

循环神经网络使用的激活函数，经常选用 $tanh$ ，不过有时候也会用 $Relu$ ，但 $tanh$ 是更通用的选择。其次我们有其它方法来避免梯度消失问题。选用哪个激活函数是取决于你的输出y。如果它是一个二分问题，可以用sigmoid作为激活函数；如果是k分类的话，可以选用softmax作为激活函数。

### 1.3.5 RNN 详细介绍之公式简化

* $a^{\left \langle t \right \rangle}=g(W_a[a^{\left \langle t-1 \right \rangle},x^{\left \langle t \right \rangle}]+b_a)$
* $\hat{y}^{\left \langle t \right \rangle}=g(W_{y}a^{\left \langle t \right \rangle}+b_y)$

#### 简化思路

定义 $W_a$ 的方式是将矩阵 $W_{aa}$ 和矩阵 $W_{ax}$ 水平放置合并起来，形式类似于 $[W_{aa}|W_{ax}]=W_{a}$ 。例如，如果a是`100`维的，x是`10000`维的，那么 $W_{aa}$ 就是个 `(100,100)` 维的矩阵，$W_{ax}$ 就是个`(100,10000)`维的矩阵，因此如果将这两个矩阵堆起来，所以 $W_a$ 就是一个`(100,10100)`维的矩阵，同样 $[a^{\left \langle t-1 \right \rangle},x^{\left \langle t \right \rangle}]$ 是一个`10100`维的向量。这种记法的好处是我们可以不使用两个参数矩阵 $W_{aa},W_{ax}$ ，而是将其压缩成一个参数矩阵 $W_a$ 。

此时下标表示计算时会输出什么类型的量。所以 $W_y$ 表示它是计算`y`类型的量的权重矩阵，而上面的 $W_a$ 则表示用来计算a类型的输出或者说是激活值的。

### 1.3.6 RNN 的缺点

在循环神经网络中，在预测 $\hat{y}^{<3>}$ 的时候，不仅要使用 $x^{<3>}$ 的信息，还要使用来自 $x^{<1>}$ 和 $x^{<2>}$ 的信息。

所以这个循环神经网络的一个**缺点**就是**只使用了这个序列中前面的信息来做出预测，没有用到后面的信息**。

* `He said, “Teddy Roosevelt was a great President.”`
* `He said, "Teddy bears are on sale!”`

以上述句子为例，为了判断`Teddy`是否是人名的一部分，仅仅知道句中前两个词，是完全不够的，还需知道句子后面的信息。

因此，有另外一种RNN结构是双向RNN，简称为BRNN ，后面会详细介绍。

## 1.4 通过时间的反向传播（Backpropagation through time）

为了进行反向传播计算，可以使用梯度下降等方法来更新RNN的参数，需要定义一个损失函数，单个元素的损失函数（交叉熵损失函数`cross entropy loss`）如下：

$L^{\left \langle t \right \rangle}(\hat{y}^{\left \langle t \right \rangle},y^{\left \langle t \right \rangle})=-y^{\left \langle t \right \rangle}log\,\hat{y}^{\left \langle t \right \rangle}-(1-y^{\left \langle t \right \rangle})log(1-\hat{y}^{\left \langle t \right \rangle})$

针对所有元素的损失函数（即成本函数）而言，公式为：

$L(\hat{y},y)=\sum_{t=1}^{T_y}L^{\left \langle t \right \rangle}(\hat{y}^{\left \langle t \right \rangle},y^{\left \langle t \right \rangle})$

反向传播（Backpropagation）过程就是从右到左分别计算成本函数对参数 $W_a，W_y，b_a，b_y$的偏导数。思路与做法与标准的神经网络是一样的。一般可以通过成熟的深度学习框架自动求导，例如PyTorch、Tensorflow等。

## 1.5 不同类型的 RNN

对于RNN，不同的问题需要不同的输入输出结构。

### 1.5.1 `many-to-many`多对多类型

该类型的第一种情况是**序列标注问题** $T_x=T_y$ ，此时输入和输出的长度相同。

第二种情况是**机器翻译问题** $T_x\not ={T_y}$ ，输入和输出都是序列，但长度却不相同，这是另外一种多对多的结构。输入序列及和输入序列有关的RNN部分被称为`encoder`，而输出序列和输出序列有关的RNN部分被称为`decoder`。后面会详细介绍。

| ![image](1.5-1%20不同类型的RNN之多对多类型.png) |
|----|

### 1.5.2 `many-to-one`多对一类型

此时 $T_x>1,T_y=1$，用于分类。如在情感分类问题中，我们要对某个序列进行正负判别或者打星操作。在这种情况下，就是输入是一个序列，但输出只有一个值：

|![image](1.5-2%20不同类型的RNN之多对一类型.png) |
|----|

### 1.5.3 `one-to-many`一对多类型

此时 $T_x<=1,T_y>1$。用于序列生成。如在音乐生成的例子中，输入一个音乐的类型或者空值，直接生成一段音乐序列或者音符序列。在这种情况下，就是输入是一个值，但输出是一个序列。这样会把第一个输出的序列传到下一层。

|![image](1.5-3%20不同类型的RNN之一对多类型.png) |
|----|

## 1.6 语言模型和序列生成

在NLP中，构建语言模型是最基础也是最重要的工作之一，我们可以通过`RNN`来很好的实现。

### 1.6.1 什么是语言模型呢

`sentence 1：The apple and pear salad was delicious.`

`sentence 2：The apple and pair salad was delicious.`

在语音识别中，很有可能将第一句话听成第二句话（单词`pear`和`pair`的发音相近）。而让语音识别系统选择第一句话的方法就是**构建语言模型，从输入的句子中，评估各个句子中各个单词出现的可能性，进而给出整个句子出现的可能性**。以上述两个句子为例，`sentence 1`就比`sentence 2`更像一个正常的句子（下面是两个句子的概率）。

$p(sentence\,1)=5.7×10^{-10}$

$p(sentence\,2)=3.2×10^{-13}$

### 1.6.2 如何建立一个语言模型

1. 构建文本语料库Corpus。包含很多文本（英文文本、中文文本等）；
2. 将文本语料库中的句子分词（Tokenize），即建立一个字典，然后将每个单词都转换成对应的one-hot向量，也就是字典中的索引。另外还要定义句子的结尾，一般就是增加一个额外的标记 $< EOS >$ `end of sentence`。如果想要模型能够准确识别句子结尾的话。可以把标记附加到训练集中每一个句子的结尾；
   * 如果训练集中有一些词并不在词典中，比如词典有10000个词，10000个最常用的英语单词。现在句子`The Egyptian Mau is a bread of cat.`中有一个词`Mau`可能并不是10000个最常用的词。这种情况下，可以把`Mau`替换成一个叫做`UNK`的代表未知词的标志。我们只针对"UNK"建立概率模型，而不是针对这个具体的词。
3. 构建一个 RNN 来计算序列的概率：
   * 画出一个 RNN 结构
   * 在第 0 个时间步，使用激活项 $a^{<0>}$ 和输入 $x^{<1>}$ 作为输入的函数计算激活项 $a^{<1>}$。此时 $a^{<0>},x^{<1>}$ 都是`0 向量`。$a^{<1>}$ 要做的是，通过一个softmax层来预测字典中的任意单词会是第一个词的概率即 $y^{<1>}$，它只是预测第一个词的概率而不去管单词的结果是什么。
   * RNN 的下一个时间步：使用激活项 $a^{<1>}$，令 $x^{<2>}=y^{<1>}$，结合上一步计算得到的 $a^{<1>}$，计算出第二个词。
   * 以此类推，直至最后的结果是EOS标志。在这一步中，通过前面得到的这些词，不管它们是什么，我们希望能预测出EOS句子结尾标志会有很高的概率。
4. 定义成本函数
   * 在某个时间步 t，如果真正的词是 $y^{< t >}$ ，而神经网络的softmax层预测的结果值是 $\hat{y}^{< t >}$ ，可以得到softmax的损失函数，即 $L(\hat{y}^{< t >},y^{< t >})=-\sum_{i=1}y^{< t >}_ilog\hat{y}_i^{< t >}$
   * 总体损失函数也就是把所有单个预测的损失函数都相加起来，可以表示为 $L = \sum_t L^{< t >}(\hat{y}^{< t >},y^{< t >})$

如果使用很大的训练集来训练这个RNN网络，就可以通过开头一系列单词，例如"cats average 15"来预测之后的单词的概率。现在有一个只含有3个单词的新句子，它是 $y^{<1>},y^{<2>},y^{<3>}$ ，现在要计算出整个句子中每个单词的概率，方法就是第一个softmax层计算出第一个输出 $P(y^{<1>})$ 的概率，然后第二个softmax会计算出在考虑 $y^{<1>}$ 的情况下 $y^{<2>}$ 的概率，即 $P(y^{<2>}|y^{<1>})$ 。第三个softmax会计算出考虑 $y^{<1>},y^{<2>}$ 的情况下 $y^{<3>}$ 的概率，也就是 $P(y^{<3>}|y^{<2>},y^{<1>})$，把这三个概率相乘，最后得到含3个词的整个句子的概率，这就是用RNN训练一个语言模型的基础结构。

## 1.7 新序列采样

在完成一个序列模型的训练之后，如果我们想要了解这个模型学到了什么，其中一种非正式的方法就是进行一次新序列采样**sample novel sequences**）

一个序列模型模拟了任意特定单词序列的概率，如 $P(y^{<1>},⋯, y^{< Ty >})$，而我们要做的就是对这个概率分布进行采样，来生成一个新的单词序列。

|![image](1.7-1%20新序列采样之RNN序列图.png) |
|----|

如上图一个已经训练好的 RNN 结构，我们已经建立了**基于词汇的RNN模型**，为了进行采样需要做的事情包括：

1. 对你想要模型生成的第一个词进行采样。令 $x^{<1>}=0, a^{<0>}=0$，在这第一个时间步，我们得到所有**可能的输出经过softmax层后可能的概率**，根据这个softmax的分布，进行**随机采样**，获取第一个随机采样单词 $y^{<1>}$出现的概率，对这个向量使用例如numpy命令，`np.random.choice`，来根据向量中这些概率的分布进行采样；
2. 下一个时间步，我们以刚刚采样得到的 $y^{<1>}$作为下一个时间步的输入，进而softmax层会预测下一个输出 $y^{<2>}$，无论你得到什么样的输出，用`one-hot`码表示的选择结果，都要把它传递到下一个时间步。
3. 依次类推直至结束。序列结束的探讨如下：
   * 如果字典中有结束的标志，如 $< EOS >$，那么输出是该符号时则表示结束；
   * 若没有这种标志，则我们可以自行设置结束的时间步。不过这种过程有时候会产生一些 `<UNK>` 标识。如果要确保算法不会输出这种标识，能做的就是拒绝采样过程中产生任何`<UNK>`的标识，一旦出现就继续在剩下的词中进行重采样，直到得到一个不是`<UNK>`标识的词；如果你不介意有未知标识产生的话，也可以完全不管它们。

根据实际的应用，还可以构建一个基于字符的RNN结构，在这种情况下，你的字典仅包含从a到z的字母(包括大小写)，可能还会有空格符和数字`0123456789`。如果建立一个基于字符的语言模型，比起基于词汇的语言模型，你的序列 $y^{<1>}, y^{<2>}, y^{<3>}$ 等在训练数据中都将是单独的字符而不是单独的词汇。所以对于上一节的例句来说， $y^{<1>},  y^{<2>}$就代表了例句中的前两个字母，等等。

使用基于字符的语言模型，**优点**就是你不必担心会出现`<UNK>`的标识。而主要的**缺点**就是会得到太多太长的序列，大多数英语句子只有10到20个单词，但却可能包含很多很多字符。所以基于字符的语言模型在捕捉句子中的依赖关系（句子较前部分如何影响较后部分）不如基于词汇的语言模型一样可以捕捉长范围的关系；同时基于字符的语言模型训练代价比较高。所以目前的趋势和常见的均是基于词汇的语言模型。但随着计算机运算能力的增强，在一些特定的情况下，也会开始使用基于字符的语言模型。

## 1.8 RNN 的梯度消失

RNN在NLP中具有很大的应用价值，但是其存在一个很大的缺陷，那就是梯度消失的问题。例如下面的例句中：

`The cat, which already ate ………，was full；`

`The cats, which already ate ………，were full.`

在这两个句子中，`cat`对应着`was`，`cats`对应着`were`，（中间存在很多很长省略的单词），句子中存在**长期依赖 long-term dependencies**（前面的单词对后面的单词有很重要的影响）。但是我们目前所见到的基本的RNN模型，是不擅长捕获这种长期依赖关系的

|![image](1.8-1%20RNN的梯度消失之标准RNN结构.png)|
|----|

如图所示，基本的深度神经网络结构中，输出`y`得到的**梯度很难通过反向传播再传播回去，也就是很难对前面几层的权重产生影响**，所以RNN也有同样的问题，也就是很难让网络记住前面的单词是单数或者复数，进而对后面的输出产生影响。 （即**因为梯度消失的问题，反向传播很困难，后面的层很难影响前面层的计算**）

也正是这个原因，所以基本的RNN模型会有很多局部影响，意味着任意输出 $\hat{y}^{< t >}$ 主要受 $\hat{y}^{< t >}$ 附近的值的影响，最后的输出基本上很难收到序列靠前的输入的影响，这是因为不管输出是什么，不管是对的还是错的，这个区域都很难反向传播到序列的前面部分，因此模型也很难调整序列前面的计算，这是基本的RNN算法的一个缺点。如果不管的话，RNN会不擅长处理长期依赖的问题。

RNN 除了会出现梯度消失问题，还会有梯度爆炸问题，事实上，梯度下降在训练RNN时是首要的问题。尽管梯度爆炸也会出现，但是梯度爆炸很明显，因为指数级大的梯度，会让你的参数变得极其大，以至于让你的网络参数崩溃，所以**梯度爆炸很容易发现，因为参数会大到崩溃**，你会看到很多NaN或者不是数字的情况，这意味着你的网络计算出现了**数值溢出**。这时的解决方法就是**梯度修剪 gradient clipping（或者 gradient clip）**（观察你的梯度向量，如果它大于某个阈值，缩放梯度向量，保证它不会太大，这就是通过一些最大值来修剪的方法）。这是相对比较鲁棒的梯度爆炸的解决方法。然而梯度消失更难解决。

下一节的 GRU 单元能够有效解决梯度消失问题，同时能够捕获更长的长期依赖

## 1.9 GRU 单元（Gated Recurrent Unit, 门控循环单元）

门控循环单元（Gated Recurrent Unit, GRU）改变了RNN的隐藏层，使其能够更好地捕捉深层连接，并改善了梯度消失的问题。

### RNN 回顾

|![image](1.9-1%20GRU单元之标准的RNN单元.png) |
|----|

在上图的RNN单元中，t 时间的激活值计算公式为： $a^{\left \langle t \right \rangle}=g(W_a[a^{\left \langle t-1 \right \rangle},x^{\left \langle t \right \rangle}]+b_a)$ ，然后该激活值会传递给softmax层来输出 $\hat{y}$ 。

### 简化的 GRU 单元

|![iamge](1.9-2%20GRU单元之简化的GRU单元图.png) |
|----|

如上图，这是一个GRU单元的图像。此时存在一个新的隐藏状态变量称为 $c$（代表cell），作为`记忆细胞`，提供了记忆能力。该变量的值与RNN中的激活值 $a$ 相等。这时会有如下公式：

* $\tilde{c}^{< t >}=tanh(W_c[c^{< t-1 >},x^{< t >}]+b_c)$ ，该候选值代替RNN中的 $a^{< t >}$ ；
* $\Gamma_u=\sigma(W_u[c^{< t-1 >},x^{< t >}]+b_u)$ ，更新门`update`。
  
  范围为`[0,1]`区间。用来决定是否用候选值更新替代当前步骤中的记忆细胞 $c^{< t >}$ ；
* $c^{< t >}=\Gamma_u*\tilde{c}^{< t >}+(1-\Gamma_u)*c^{< t-1 >}$ ，细胞的更新规则；
  
  **说明**：
  
  当更新门的值 $\Gamma_u=1$，那么最终传递给下一步的激活值相当于原来RNN的激活值。
  
  当更新门的值 $\Gamma_u=0$ ，即 $W_u[c^{< t-1 >},x^{< t >}]+b_u$ 取到一个很大的负数时，相当于不更新激活值，使用上一步的值  $c^{< t >}=c^{< t-1 >}$。（该说明中的`等于号`指的是`无限接近`）。这非常有利于维持记忆细胞的值，因为 $\Gamma_u$ 很接近0，导致 $c^{< t >}=c^{< t-1 >}$ ，而且 $c^{< t >}$ 的值也很好地被维持了，即使经历了很多很多的时间步，这就是缓解梯度消失的关键。因此允许神经网络运行在非常庞大的依赖词上.

* 上述公式中的 $c^{< t >}, \tilde{c}^{< t >}, \Gamma_u$ 具有相同的维度。

### 完整的 GRU 单元

完整的GRU单元，要做的一个改变就是在上一小节`简化的GRU单元`的第一个式子中，给记忆细胞的新候选值加上一个新的项，添加一个门 $\Gamma_r$ ，即 $\tilde{c}^{< t >}=tanh(W_c[\Gamma_r*c^{< t-1 >},x^{< t >}]+b_c)$ 。

这里的`r`代表相关性， $\Gamma_r$ 会告诉你要计算的 $\tilde{c}^{< t >}$ 和上一个激活值 $c^{< t-1 >}$ 有多大的相关性，相关性门的计算公式为

* $\Gamma_r=\sigma(W_r[c^{< t-1 >},x^{< t >}]+b_c)$

### 总结

完整的GRU单元的计算公式如下所示：

* $\Gamma_r=\sigma(W_r[c^{< t-1 >},x^{< t >}]+b_c)$ ，相关性门
* $\Gamma_u=\sigma(W_u[c^{< t-1 >},x^{< t >}]+b_u)$ ，更新门
* $\tilde{c}^{< t >}=tanh(W_c[\Gamma_r*c^{< t-1 >},x^{< t >}]+b_c)$ ，候选值
* $c^{< t >}=\Gamma_u*\tilde{c}^{< t >}+(1-\Gamma_u)*c^{< t-1 >}$ ，输出的激活值（下一个单元的输入激活值）
  
## 1.10 LSTM （long short-term memory, 长短期记忆）

GRU能够让我们在序列中学习到更深的联系，长短期记忆（long short-term memory, LSTM）对捕捉序列中更深层次的联系要比GRU更加有效。LSTM中，使用了单独的更新门 $Γ_u$ 、遗忘门 $Γ_f$，和输出门 $Γ_o$，其主要的公式如下：

* $\tilde{c}^{< t >}=tanh(W_c[a^{< t-1 >},x^{< t >}]+b_c)$ ，候选值
* $\Gamma_u=\sigma(W_u[c^{< t-1 >},x^{< t >}]+b_u)$ ，更新门
* $\Gamma_f=\sigma(W_f[c^{< t-1 >},x^{< t >}]+b_f)$ ，遗忘门
* $\Gamma_o=\sigma(W_o[c^{< t-1 >},x^{< t >}]+b_o)$ ，输出门
* $c^{< t >}=\Gamma_u*\tilde{c}^{< t >}+\Gamma_f*c^{< t-1 >}$ ，输出的激活值（下一个单元的输入激活值）
* $a^{< t >}=\Gamma_o*c^{< t >}$

LSTM 单元的具体计算流程如下图所示：

|![image](1.10-1%20LSTM之LSTM单元图.png) |
|----|

以上就是LSTM，你可能会想到，这里和一般使用的版本会有所不同，最常用的版本可能是，`做窥视孔连接`，其实意思就是门值不仅仅取决于 $a^{< t-1 >}$ 和 $x^{< t >}$ ，也取决于上一个记忆细胞的值 $c^{< t-1 >}$ ，然后`窥视孔连接`就可以结合这三个门来计算了。而这三个门只在自身单元内起作用，不会影响其他单元的计算

## 1.11 双向神经网络

有两个方法可以构建更好的模型，第一个就是双向RNN模型，这个模型可以让你在序列的某点处，不仅可以获取之前的信息，还可以获取未来的信息；第二个就是深层的RNN，会在之后进行介绍。

`He said, “Teddy bears are on sale”`

`He said, “Teddy Roosevelt was a great President”`

在上述命名实体识别的两个句子中，如果使用单向的RNN（从左到右），在判断第三个词`Teddy`是不是人名的一部分时，光看句子前面部分是不够的。为了判断 $\hat{y}^{<3>}$ 是0还是1，除了前3个单词，还需要更多的信息，不管这些单元是标准的RNN、GRU单元、LSTM，根据前3个单词都无法判断他们说的`Teddy`是熊还是前总统，因为这些构建都是只有前向的。单向RNN无法实现获取未来的信息。

|![image](1.11-1%20双向神经网络之BRNN图像.png) |
|----|

以只有4个输入单词的句子为例。首先依次计算前向序列（从左到右）的激活值 $a^{<1>},a^{<2>},a^{<3>},a^{<4>}$ ，而反向序列（从右到左，这里不是反向传播）依次计算激活值 $a^{<4>},a^{<3>},a^{<2>},a^{<1>}$。图中这个前向传播一部分计算是从左到右，一部分计算是从右到左。把所有这些激活值都计算完后，就可以计算预测结果。预测结果的具体公式为：

* $\hat{y}^{< t >}=g(W_y[\overrightarrow{a}^{< t >},\overleftarrow{a}^{< t >}]+b_y)$

双向循环神经网络的基本单元不仅仅是标准RNN单元，也可以是GRU单元或者是LSTM单元。在这些单元中，LSTM单元的双向模型是用的最多的，也称为`BiLSTM`。

双向RNN的缺点就是需要完整的数据的序列才能预测任意位置。构建语音识别系统时，双向RNN模型就需要考虑整个语音表达，即要说完一段语音才能识别

## 1.12 深层循环神经网络

与深层的基本神经网络结构相似，深层RNNs模型具有多层的循环结构，但不同的是，在传统的神经网络中，我们可能会拥有很多层，几十层上百层，但是对与RNN来说，三层的网络结构就已经很多了，因为RNN存在时间的维度，所以其结构已经足够的庞大。如下图所示：

|![image](1.12-1%20深层循环神经网络之深层网络图.png) |
|----|

此时 $a^{[l]< t >}$ 的 $l$ 表示第 $l$ 层，$t$ 表示第 $t$ 个时间点。

**注意**：深层的RNN模型训练需要很多计算资源，需要很长时间。
