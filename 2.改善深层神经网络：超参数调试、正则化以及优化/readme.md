## 改善深层神经网络：超参数调试、正则化以及优化

1. [深度学习的实用层面（基本概念和方法）](1.深度学习的实用层面.md)
    - 1.1 训练/验证/测试集`train/dev/test sets`
    - 1.2 偏差`Bias`/ 方差`Variance`
    - 1.3 基本优化方法
    - 1.4 正则化及其作用原理
    - 1.5 正则化减少过拟合的原因
    - 1.6 `Dropout`正则化
    - 1.7 其他正则化方法
    - 1.8 正则化输入
    - 1.9 梯度消失与梯度爆炸
    - 1.10 神经网络的权重初始化
    - 1.11 梯度检验（确保反向传播的正确）
    - 1.12 梯度检验的补充
2. [优化算法](2.优化算法.md)
    - 2.1 mini-batch 梯度下降法
    - 2.2 理解 mini-batch 梯度下降法
    - 2.3 指数加权平均 Exponentially weighted averges
    - 2.4 动量梯度下降法 Gradient Descent with Momentum
    - 2.5 RMSprop 算法
    - 2.6 Adam 优化算法
    - 2.7 学习率衰减`learning rate decay`
    - 2.8 局部最优问题
3. [超参数调试、Batch正则化、程序框架](3.超参数调试、Batch正则化、程序框架.md)
    - 3.1 调试处理（调参）Tuning Process
    - 3.2 超参数的合适范围
    - 3.3 超参数训练的实践
    - 3.4 归一化网络的激活函数（重要！！！）Batch Norm（BN）
    - 3.5 神经网络中的 Batch_Norm（将 Batch_Norm 拟合进神经网络）
    - 3.6 Batch Norm 奏效的原因
    - 3.7 测试时的Batch Norm
    - 3.8 Softmax 回归
    - 3.9 深度学习框架与 Tensorflow
