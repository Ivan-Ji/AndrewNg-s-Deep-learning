<!-- TOC -->

- [2.优化算法`Optimization Algorithms`](#2优化算法optimization-algorithms)
    - [2.1 mini-batch 梯度下降法](#21-mini-batch-梯度下降法)
        - [在深度学习中，会使用花括号的上标 $X^{\left\{t\right\}}$ 表示不同的 mini-batch](#在深度学习中会使用花括号的上标-x^\left\t\right\-表示不同的-mini-batch)
    - [2.2 理解 mini-batch 梯度下降法](#22-理解-mini-batch-梯度下降法)
        - [2.2.1 成本函数 $J$ 在两种极端情况下的优化情况](#221-成本函数-j-在两种极端情况下的优化情况)
        - [2.2.2 如果 mini-batch 的大小既不是 1 也不是 m ，取中间值应该怎么选择呢](#222-如果-mini-batch-的大小既不是-1-也不是-m-取中间值应该怎么选择呢)
            - [mini-batch是一个非常重要的超参数，一般64，128，256，512选其一](#mini-batch是一个非常重要的超参数一般64128256512选其一)
    - [2.3 指数加权平均 Exponentially weighted averges](#23-指数加权平均-exponentially-weighted-averges)
        - [2.3.1 指数加权平均的概念](#231-指数加权平均的概念)
        - [2.3.2  $β$ 如何选择呢](#232--β-如何选择呢)
        - [2.3.3 指数加权平均的偏差修正](#233-指数加权平均的偏差修正)
        - [偏差修正的办法](#偏差修正的办法)
    - [2.4 动量梯度下降法 Gradient Descent with Momentum](#24-动量梯度下降法-gradient-descent-with-momentum)
    - [2.5 RMSprop 算法](#25-rmsprop-算法)
        - [公式原理](#公式原理)
    - [2.6 Adam 优化算法](#26-adam-优化算法)
        - [Adam 算法名字的由来](#adam-算法名字的由来)
    - [2.7 学习率衰减`learning rate decay`](#27-学习率衰减learning-rate-decay)
        - [其他学习率衰减公式](#其他学习率衰减公式)
    - [2.8 局部最优问题](#28-局部最优问题)
        - [如果局部最优不是问题，那么问题是什么呢](#如果局部最优不是问题那么问题是什么呢)
        - [要点](#要点)

<!-- /TOC -->

# 2.优化算法`Optimization Algorithms`

机器学习的应用是一个高度依赖经验的过程，伴随大量迭代过程。但深度学习没有在大数据领域发挥最大的效果，我们可以利用一个巨大的数据集来训练神经网络，而在巨大的数据集上进行训练，速度会很慢。因此，使用好且快速的优化算法能够大大提高机器学习和神经网络的效率。以下是几种在深度学习里常用的几种优化算法。

## 2.1 mini-batch 梯度下降法

向量化能有效地对所有`m`个例子进行计算，可以处理整个训练集而无需某个明确的公式，所以我们要把训练样本放到巨大的矩阵中，如下所示：

$X=\left [ x^{(1)},x^{(2)},x^{(3)}...x^{(m)} \right ]$

$Y=\left [ y^{(1)},y^{(2)},y^{(3)}...y^{(m)} \right ]$

这里 $X$ 的维度为 $(n_{x},m)$ ，$Y$ 的维度为 $(1,m)$ 。向量化能相对较快地处理所有 m 个样本，但如果 m 很大的话，处理速度仍然缓慢，比如说 m 是500万或者5000万或者更大的一个数，在对整个训练集执行梯度下降法时，必须处理整个训练集，然后才能进行一步梯度下降法，然后需要再重新处理500万个训练样本才能进行下一步梯度下降法。如果在处理完整个500万个样本的训练集之前，先让梯度下降法处理一部分，算法速度会更快。

可以把训练集分割为小一点的子训练集，这些子集被取名为 mini-batch，假设每一个子集中只有1000个样本，那么将其中的 $x^{(1)}$ 到 $x^{(1000)}$ 取出来，即 $X=\left [ x^{(1)},x^{(2)},x^{(3)}...x^{(1000)} \right ]$，将其称之为第一个子训练集，也叫做`mini-batch`，然后再取出接下来的1000个样本，即 $x^{(1001)}$ 到 $x^{(2000)}$ 。

mini-batch的原理如下所示：

```text
for t=1,2,3 ... 5000:
    在X{t}上进行前向传播：
        Z[1]=W[1]X{t}+b[i]
        A[1]=g[1](Z[1])
        ... ...
        A[L]=g[L](Z[L])         # 上面都是向量化的操作
    计算成本函数J；
    反向传播计算梯度；
    更新参数W和b；
```

视频里讲到了几个名词，添加和扩展后如下所示：

1. **batch**：批次。将整个训练样本分成若干个 batch。上述例子的batch数量为5000；
2. **batch_size**：批大小。每次训练在训练集中取batch_size个样本训练。上述例子的batch_size数量为1000；
3. **iteration**：迭代。1个iteration等于使用batch_size个样本训练一次；
4. **epoch**：代。使用训练集的全部数据对模型进行一次完整训练，被称为`一代训练`。

### 在深度学习中，会使用花括号的上标 $X^{\left\{t\right\}}$ 表示不同的 mini-batch

如果训练集数据有500万，每个mini-batch都有1000个样本，也就是说有5000个mini-batch。

一般的梯度下降法一次遍历训练集只能做一次梯度下降，而使用mini-batch梯度下降法，遍历一次训练集就可以做5000个梯度下降。所以**mini-batch梯度下降法比一般的梯度下降法运行更快**。

## 2.2 理解 mini-batch 梯度下降法

使用普通的梯度下降法时，每次迭代都需要遍历整个训练集，可以预期每次迭代的成本都会下降。如果成本函数J是迭代次数的一个函数，它应该会随着每次迭代而减少，但如果成本函数J在某次迭代中增加了，一定出了问题。

如果使用mini-batch梯度下降法，如果作出成本函数在整个过程中的图，则并不是每次迭代都是下降的，特别是在每次迭代中要处理的是 $X^{\left\{t\right\}}$ 和 $Y^{\left\{t\right\}}$ 。此时如果要作出成本函数 $J^{\left\{t\right\}}$ 的图，而 $J^{\left\{t\right\}}$ 只和 $X^{\left\{t\right\}}$ 和 $Y^{\left\{t\right\}}$ 有关，也就是每次迭代下都在训练不同的样本数或者说训练不同的mini-batch，很可能是下图中的结果，走向朝下，但有更多的噪声。

|![image](2.2-1%20mini-batch梯度下降之成本函数图像.png)|
|----|

假如在训练mini-batch梯度下降法时，$J^{\left\{t\right\}}$ 的图没有每次迭代都下降，这不要紧，但是走势应该向下，噪声产生的原因在于，可能在mini-batch训练中 $X^{\left\{1\right\}}$ 和 $Y^{\left\{1\right\}}$ 是比较容易计算的，因此成本低一点。而 $X^{\left\{2\right\}}$ 和 $Y^{\left\{2\right\}}$ 可能是比较难运算的，或许你需要一些残缺的样本，这样一来成本会更高一些，所以才会出现这些摆动，因为是在运行mini-batch梯度下降法作出的成本函数图（上图所示）。

需要决定的变量之一是mini-batch的大小，m是训练集大小。这里有两种极端情况：

1. 如果batch_size为 m，其实就是普通的batch梯度下降法。在这种极端情况下，就有 $X^{\left\{1\right\}}$ 和 $Y^{\left\{1\right\}}$ ，并且该mini-batch等于整个训练集，所以把batch_size设为m可以得到batch梯度下降法；
2. 假设batch_size为1，这就是**随机梯度下降法**，每个样本都是独立的mini-batch。当看第一个mini-batch，即 $X^{\left\{1\right\}}$ 和 $Y^{\left\{1\right\}}$ ，它就是第一个训练样本。后面的以此类推。

### 2.2.1 成本函数 $J$ 在两种极端情况下的优化情况

|![image](2.2-2%20mini-batch梯度下降之成本函数轮廓图.png)|
|----|

如果上图是想要最小化的成本函数的轮廓，最小值点在轮廓中心。

1. **batch 梯度下降法**（蓝色线条）

   从某处开始，相对噪声低些，幅度也大一些；

2. **随机梯度下降法**（紫色线条）

   从某一点开始，每次迭代只对一个样本进行梯度下降，大部分时候向着全局最小值靠近。有时候会远离最小值，因为那个样本恰好给你指定的方向不对。因此 **随机梯度下降法有很多噪声** 。总的来说，它最终会靠近最小值，不过有时候也会方向错误。因为随机梯度下降法永远不会收敛，而是会一直在最小值附近波动，但它并不会在达到最小值并停留在最小值。

3. **mini-batch 梯度下降法**（绿色线条）

    它不会总朝向最小值靠近，但它比随机梯度下降要更持续地靠近最小值的方向，它也不一定在很小的范围内收敛或者波动，如果出现这个问题，可以慢慢减少学习率。

实际上，**mini-batch 大小需要在 1 和 m 二者之间**，原因如下：

1. 如果使用 **batch 梯度下降法**，mini-batch的大小为m，每次迭代需要处理大量训练样本。主要缺点在于，特别在训练样本数量巨大的时候，单次迭代耗时太长；

2. 相反，如果使用 **随机梯度下降法**，如果只要处理一个样本，这个方法很好，通过减小学习率，噪声会被改善或者有所减少，但该方法的一大缺点是会失去向量化带来的加速效果，因为每一次只处理一个训练样本，效率过于低下。

因此，实践中最好选择不大不小的batch_size，这会使深度学习变得更快。这样做有**两个好处**：1. 得到了大量向量化的数据；2. 不需要等待整个训练集被处理完就可以开始后续工作。

### 2.2.2 如果 mini-batch 的大小既不是 1 也不是 m ，取中间值应该怎么选择呢

首先，如果训练集较小，直接用batch梯度下降法，样本集较小就没必要使用mini-batch梯度下降法，可以快速处理整个训练集，所以使用batch梯度下降法也很好，这里说的少是说小于2000个样本，这样比较适合使用batch梯度下降法。

如果样本数目较大的话，一般的mini-batch大小为64到512，考虑到电脑内存设置和使用的方式，如果mini-batch大小是2的幂，代码会运行地快一些。

最后要注意的是，在mini-batch中，$X^{\left\{t\right\}}$ 和 $Y^{\left\{t\right\}}$ 要符合CPU/GPU内存。这取决于应用方向，以及训练集的大小。如果处理的mini-batch和CPU/GPU内存不相符，不管用什么方法处理数据，你会注意到算法的表现急转直下。

#### mini-batch是一个非常重要的超参数，一般64，128，256，512选其一

## 2.3 指数加权平均 Exponentially weighted averges

### 2.3.1 指数加权平均的概念

**指数加权平均**也叫**指数加权移动平均**，是一种常用的序列数据处理方式。它的计算公式如下：

$v_t=\beta v_{t-1}+(1-\beta)\theta_t$

其中：

* $θ_t$ ：为第 t 天的实际观察值；
* $v_t$ : 是要代替 $θ_t$ 的估计值，也就是第 t 天的指数加权平均值；
* $β$ ： 为 $v_{t-1}$ 的权重，是可调节的超参。( 0 < $β$ < 1 )

例如，我们有这样一组气温数据，下图中横轴为一年中的第几天，纵轴为气温：

|![image](2.3-1%20指数加权平均之气温散点图.png)|
|----|

直接看上面的数据图会发现噪音很多，这时，我们可以用 **指数加权平均** 来提取这组数据的趋势。按照前面的公式计算：这里先设置 $β=0.9$ ，首先初始化 $v_0$ = 0，然后计算出每个 $v_t$，可以得到如下公式：

$v_0=0\\v_1=\beta v_0+(1-\beta)\theta_1\\v_2=\beta v_1+(1-\beta)\theta_2\\......\\v_t=\beta v_{t-1}+(1-\beta)\theta_t$

将计算后得到的 $V_t$ 表示出来，就得到了下图中红色线条的数值：

|![image](2.3-2%20指数加权平均之红色拟合图.png) |
|----|

可以看出，红色的数据比蓝色的原数据更加平滑，不仅少了很多噪音，并且刻画了原数据的趋势。指数加权平均，作为原数据的估计值，可以抚平短期波动，起到了平滑的作用。

### 2.3.2  $β$ 如何选择呢

根据前面的公式：

$v_0=0\\v_1=\beta v_0+(1-\beta)\theta_1\\v_2=\beta v_1+(1-\beta)\theta_2\\......\\v_t=\beta v_{t-1}+(1-\beta)\theta_t$

即：

```text
v = 0
repeat {
    get next θ_t
    v_t = β * v_(t-1) + (1-β) * θ_t
}
```

将 $v_{100}$ 展开得到：$v_{100}=0.1\theta_{100} +0.9\theta_{99}=0.1\theta_{100} +0.9(0.1\theta_{99}+0.9\theta_{98})= ... =0.1\theta_{100}+0.1*0.9\theta_{99}+0.1*0.9^2\theta_{98}+...+0.1*0.9^{99}\theta_{1}$

这里可以看出，$v_t$ 是对每天温度的加权平均，之所以称之为指数加权，是因为**加权系数是随着时间以指数形式递减的，时间越靠近，权重越大，越靠前，权重越小**。所有这些系数加起来为 1 或者逼近 1 ，我们称之为**偏差修正**。因为有偏差修正，才是指数加权平均数。

当 $\beta=0.9$ 时，$0.9^{10}=0.3486\approx\frac{1}{e}=0.3678$，假如我们以 $\frac{1}{e}$ 为一个分界点，当权重衰减到比 $\frac{1}{e}$ 小的时候，就可以忽略不计。则对任意的 $\beta$ ， $\beta^{\frac{1}{1-\beta}}\approx\frac{1}{e}$ ，所以指数加权平均就是最近 $N=\frac{1}{1-\beta}$ 天的平均值。

10天后曲线的高度下降到三分之一，相当于在峰值的 $\frac{1}{e}$ 。当 $\beta=0.9$ 的时候，仿佛在计算一个指数加权平均数，只关注过去10天的温度，因为10天后，权重下降到不到当日权重的三分之一。

相反，如果 $\beta=0.98$，$0.98$ 的50次方大约等于 $\frac{1}{e}$。所以前50天这个数值比 $\frac{1}{e}$ 大，数值会快速衰减，所以本质上这是一个下降幅度很大的函数，可以看做平均50天的温度。

来看下面三种情况：

* 当 $β = 0.9$ 时，指数加权平均最后的结果如`2.3.1`图中红色线所示，代表的是最近 10 天的平均温度值；
* 当 $β = 0.98$ 时，指结果如图绿色线所示，代表的是最近 50 天的平均温度值；
* 当 $β = 0.5$ 时，结果如下图黄色线所示，代表的是最近 2 天的平均温度值；

|![image](2.3-3%20指数加权平均之黄绿拟合图.png) |
|----|
由此可以看出：

* $β$ 越小，噪音越多，虽然能够很快的适应温度的变化，但是更容易出现奇异值。
* $β$ 越大，得到的曲线越平坦，因为多平均了几天的温度，这个曲线的波动更小。但有个缺点是，因为只有 0.02 的权重给了当天的值，而之前的数值权重占了 0.98 曲线进一步右移，在温度变化时就会适应地更缓慢一些，会出现一定延迟。

指数加权平均的好处之一在于只占用极少的内存（只占用一行数字而已），然后把最新数据代入公式，不断覆盖即可。当然它并不是最好的，也不是最精准的计算平均数的方法，如果要移动窗口，直接算出过去10天甚至50天的总和并求平均即可，往往会得到更好的估测，但缺点是，如果保存所有最近10天或50天的数据，必定占用更多的内存、执行更加复杂，计算成本更高。

通过上面的内容可知，$β$ 也是一个很重要的超参数，不同的值有不同的效果，需要调节来达到最佳效果，**一般 0.9 的效果就很好**。

### 2.3.3 指数加权平均的偏差修正

| ![image](2.3-4%20指数加权平均之偏差修正图.png) |
|----|

$v_t=βv_{t-1}+(1-β)θ_t$

在上一个笔记中，红色曲线对应 $β$ 的值为 $0.9$ ；绿色曲线对应的 $β=0.98$ ；而 $β=0.98$ 的时候，得到的是紫色曲线。可以注意到紫色曲线的起点较低，这应该怎么处理呢

计算移动平均数的时候，初始化 $v_0=0，v_1=0.98v_0+0.02θ_1$，但是由于 $v_0=0$，所以 $0.98v_0$ 的部分没有了，所以 $v_1=0.02θ_1$ 。如果一天温度是40华氏度，那么 $v_1=0.02θ_1=0.02×40=8$，因此得到的值会小很多，所以第一天温度的估测不准。

$v_2=0.98v_1+0.02θ_2=0.98×0.02θ_1+0.02θ_2=0.0196θ_1+0.02θ_2$，假设 $θ_1$ 和 $θ_2$ 都是正数，计算后 $v_2$ 要远小于 $θ_1$ 和 $θ_2$ ，所以 $v_2$ 不能很好估测出这一年前两天的温度。

### 偏差修正的办法

在估测初期。也就是不用 $v_t$ ，而是用 $\frac{v_t}{1-\beta^t}$ 来代替 $v_t$ ，公式中的 $t$ 是指现在的天数。

举个具体例子，当 $t=2$ 时，$1-β^t=1-0.98^2=0.0396$，因此对第二天温度的估测变成了 $v_2/0.0396=(0.0196θ_1+0.02θ_2)/0.0396$ ，也就是 $θ_1$ 和$θ_2$的加权平均数，并去除了偏差。

随着 $t$ 增加， $β^t$ 接近于 $0$，所以当 $t$ 很大的时候，偏差修正几乎没有作用，因此当 $t$ 较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，刚开始预测热身练习，偏差修正可以帮助你更好预测温度，使结果从紫线变成绿线。

在机器学习中，在计算指数加权平均数的大部分时候，大家不在乎执行偏差修正，因为大部分人宁愿熬过初期，拿到具有偏差的估测，然后继续计算下去。如果你关心初始时期的偏差，在刚开始计算指数加权移动平均数的时候，偏差修正能帮助你在初期获取更好的估测。

通过计算 **指数加权移动平均**，就可以构建更好的优化算法。

## 2.4 动量梯度下降法 Gradient Descent with Momentum

**动量梯度下降法** 运行速度快于标准的梯度下降法。基本思想为：计算梯度的指数加权平均数，并利用该梯度更新权重。

如果要优化成本函数，函数如下图所示，红点代表最小值的位置。假设从左下角的边缘开始梯度下降，蓝色曲线就是一步步的迭代。可以看出这种上下波动，减慢了梯度下降法的速度，而且无法使用更大的学习率，这会导致花费更多的时间。同时为了避免摆动过大，需要用一个较小的学习率。因为如果用较大的学习率，可能会偏离函数的范围。

| ![image](2.4-1%20动量梯度下降之成本函数形状.png) |
|----|

另一个看待问题的角度是，在竖轴上我们希望摆动小一点，同时，在横轴上希望摆动能大一点，所以我们使用Momentum梯度下降法。我们需要做的是，在每次迭代中，确切来说在第 $t$ 次迭代的过程中，需要计算微分 $dw$ 、$db$ 。

根据**指数加权平均**的公式： $v_t=βv_{t-1}+(1-β)θ_t$

可以有如下公式：

* $V_{dw}=β∗V_{dw}+(1−β)∗dW$

* $V_{db}=β∗V_{db}+(1−β)∗db$

然后使用 $V_{dw}$ 和 $V_{db}$ 更新权重：

* $w=w-\alpha*V_{dw}​$

* $b=b-\alpha*V_{db}$

这样就可以减缓梯度下降的幅度

| ![image](2.4-2%20动量梯度下降之优化后成本函数图.png) |
|----|

由上图可以看出

* 纵轴方向，平均过程中正负摆动相互抵消，平均值接近于零，摆动变小，学习放慢。
* 横轴方向，因为所有的微分都指向横轴方向，因此平均值仍然较大，向最小值运动更快了。
在抵达最小值的路上减少了摆动，加快了训练速度。

Momentum 的一个本质，就是 Momentum 能够最小化碗状函数， $dW$ 和 $db$ 可以想象它们为从山上往下滚的一个球，提供了加速度，Momentum 项就相当于速度。想象有一个碗，拿一个球，微分给了这个球一个加速度，此时球正向山下滚，球因为加速度越滚越快，而因为 $\beta$ 稍小于1，表现出一些摩擦力，所以球不会无限加速下去。所以不像梯度下降法每一步都独立于之前的步骤，球可以向下滚，获得动量。

算法有两个超参数，学习率 $\alpha$ 和参数 $β$ ， $β$ 控制着指数加权平均数， $β$ 最常用的值是 0.9，平均了前十次迭代的梯度。实际上 $β$ 为 0.9 时就可以取得很好的效果。

实际上这里并不使用偏差修正。因为10次迭代之后，移动平均已经过了初始阶段，不再是一个具有偏差的预测。实际上，在使用梯度下降法或者 Momentum 时，不会受到偏差修正的困扰。当然 $V_{dw}$ 的初始值为0。

在一些情况下，动量梯度下降的公式会变为：

* $V_{dw}=β∗V_{dw}+dW$

* $V_{db}=β∗V_{db}+db$

相当于整个式子乘以了 $\frac{1}{1-\beta}$ ，此时学习率 $\alpha$ 就需要根据 $\frac{1}{1-\beta}$ 进行相应变化，比较麻烦。所以一般情况下不使用式子乘以了 $\frac{1}{1-\beta}$ 的方法。

## 2.5 RMSprop 算法

**RMSprop算法** 全称是root mean square prop算法（均方根传播），该算法可以加速梯度下降，回忆一下之前的例子，如果执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅度的摆动，假设纵轴代表参数 $b$，横轴代表参数$W$，可能有 $W_1$ ，$W_2$ 或者其它重要的参数，为了便于理解，称为 b 和 W 。所以如果想减缓b方向的学习，同时加快横轴方向的学习，RMSprop算法可以实现这一点。

| ![image](2.5-1%20RMSprop之优化示意图.png) |
|----|

在第 $t$ 次迭代中，该算法会照常计算当下mini-batch的微分 $dW$ 和 $db$ 。这里用新符号 $S_{dw}$ ，

* $S_{dw}=β∗S_{dw}+(1−β)∗(dW)^2$

* $S_{db}=β∗S_{db}+(1−β)∗(db)^2$

公式中平方的操作能够保留微分平方的加权平均数。而更新参数的公式变为：

* $W=W−α∗\frac{dW}{\sqrt{S_{dW}}}$

* $b=b-\alpha*\frac{db}{\sqrt{S_{db}}}$

### 公式原理

在横轴方向或者在例子中的W方向，我们希望学习速度快，而在垂直方向，也就是例子中的b方向，我们希望减缓纵轴上的摆动。所以有了 $S_{dW}$ 和 $S_{db}$ ，我们希望 $S_{dW}$ 会相对较小，所以 $W$ 参数更新要除以一个较小的数，而希望 $S_{db}$ 较大，这样 $b$ 更新会除以一个较大的数字，这样就可以减缓纵轴上的变化。

RMSprop的影响就是，纵轴方向上的摆动较小，而横轴方向继续推进。还有个影响就是，可以用更大学习率 $\alpha$ 加快学习。

在RMSprop中要确保算法不会除以 0，如果 $S_{dW}$ 的平方根趋近于0怎么办？这样得到的结果非常大，为了确保数值稳定，在实际中操作的时候，要在分母加上一个很小很小的 $ε$ ， $ε$ 是多少没关系，选择 $10^{-8}$ 会很不错，这只是保证数值能够稳定一些。无论什么原因，都不会除以一个很小很小的数。

RMSprop跟Momentum有很相似的一点，可以消除梯度下降中的摆动，并允许使用一个更大的学习率 $\alpha$，从而加快算法学习速度。

## 2.6 Adam 优化算法

**Adam算法**是Momentum和RMSprop结合在一起得到的。

使用Adam算法，首先要初始化 $V_{dw}=0,V_{db}=0,S_{dw}=0,S_{db}=0$。在第 t 次迭代中，要计算微分，用当前的mini-batch计算 $dW,db$ ，一般会用mini-batch梯度下降法，接下来计算momentum指数加权平均数：

* $V_{dw}=β_1∗V_{dw}+(1−β_1)∗dW$

* $V_{db}=β_1∗V_{db}+(1−β_1)∗db$

接着用RMSprop进行更新:

* $S_{dw}=β_2∗S_{dw}+(1−β_2)∗(dW)^2$

* $S_{db}=β_2∗S_{db}+(1−β_2)∗(db)^2$

一般使用Adam算法的时候，要计算偏差修正：

* $V_{dw}^{corrected}=\frac{V_{dw}}{1-\beta_1^t}$

* $V_{db}^{corrected}=\frac{V_{db}}{1-\beta_1^t}$

* $S_{dw}^{corrected}=\frac{S_{dw}}{1-\beta_2^t}$

* $S_{db}^{corrected}=\frac{S_{db}}{1-\beta_2^t}$

最后更新权重：

* $W=W-\alpha\frac{V_{dw}^{corrected}}{\sqrt{S_{dw}^{corrected}}+\varepsilon}$​

* $b=b-\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon}$

所以Adam算法结合了Momentum和RMSprop梯度下降法，并且是一种及其常见的学习算法，被证明能有效适用于不同的神经网络。

该算法有很多很重要的超参数：

* 学习率 $\alpha$ 很重要，也经常需要调试，可以尝试一系列值，然后看哪个有效。

* $\beta_1$ 常用的默认值为$0.9$，这是 $dW$ 的移动平均数，也就是 $dW$ 的加权平均数，这是Momentum涉及的项。
* $\beta_2$ ，Adam算法的发明者推荐使用 $0.999$，这是在计算 $(dW)^2$ 以及 $(db)^2$ 的移动加权平均值。

* $ε$ 的选择其实没有很重要，Adam论文的作者建议 $ε=10^{-8}$。不过一般不需要设置，因为它不会影响算法表现。

一般来说只需要调整学习率 $\alpha$ 即可。

### Adam 算法名字的由来

Adam代表`Adaptive Moment Estimation`，$\beta_1$ 用于计算微分，被称作**第一矩**。$\beta_2$ 用来计算平方数的指数加权平均数，被称作**第二矩**。Adam的名字由此而来。一般简称为**Adam权威算法**（Adam authorization algorithm）

## 2.7 学习率衰减`learning rate decay`

加快学习算法的一个办法就是随着时间慢慢减少学习率，我们将之称为学习率衰减。

假设要使用mini-batch梯度下降法，mini-batch 数量不大，大概64或者128个样本。在迭代过程中会有噪声，迭代会不断向最小值下降，但是不会精确地收敛，所以算法最后在最小值点附近摆动，并不会真正地收敛。这是因为用的 $\alpha$ 是固定值，但是不同的mini-batch中有噪音。不过要慢慢减少学习率 $\alpha$ 的话，在初期的时候 $\alpha$ 学习率还较大，学习还是相对较快，但是随着 $\alpha$ 变小，步伐也会变慢变小。所以最后曲线会在最小值附近的一小块区域摆动，而不是在训练过程中大幅度地在最小值附近摆动。

所以慢慢减少 $\alpha$ 的本质在于，在学习初期能承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。

如果初始学习率为 $\alpha_0$，我们可以将学习率 $\alpha$ 设为：

* $\alpha=\frac{1}{1+decay_{rate}*epoch_{num}}*\alpha_0$

注意公式中的**衰减率** $decay_{rate}$ 是另一个需要调整的超参数。举一个具体的例子，设 $\alpha_0=0.2$，衰减率 $decay_{rate}=1$，那么在第一个epoch中代入公式中可以得到 $\alpha_1=\frac{1}{1+1*1}*0.2=0.1$ 。

根据学习率更新公式，学习率呈递减趋势。如果想学习率衰减，要做的是尝试不同的值，包括超参数 $\alpha_0 $以及超参数衰减率，找到合适的值。

### 其他学习率衰减公式

除了这个学习率衰减的公式，还可以用其它的公式。

* 指数衰减 $\alpha=0.95^{epoch_{num}}*\alpha_0$，其中0.95为自定义的超参数

* $\alpha=\frac{k}{\sqrt{epoch_{num}}}*\alpha_0$，其中 $k$ 为常数的超参数

* 离散下降的学习率：不同的步骤有不同的学习率

## 2.8 局部最优问题

在深度学习研究早期，人们总是担心优化算法会困在极差的局部最优，不过随着深度学习理论不断发展，我们对局部最优的理解也发生了改变。那现在怎么看待局部最优以及深度学习中的优化问题呢？

下图是人们在想到局部最优时脑海里会出现的图，也许你想优化一些参数，我们把它们称之为 $W_1$ 和 $W_2$ ，平面的高度就是损失函数。

| ![image](2.8-1%20局部最优之成本函数图.png) |
|----|

在图中似乎各处都分布着局部最优。梯度下降法或者某个算法可能困在一个局部最优中，而不会抵达全局最优。如果要作图计算一个数字，比如说这两个维度，就容易出现有多个不同局部最优的图.事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点（二阶导数为0）。

但是一个具有高维度空间的函数，如果梯度为0，那么在每个方向，它可能是凸函数，也可能是凹函数。如果你在 $20000$ 维空间中，想要得到局部最优，所有的2万个方向都需要是这样，但发生的机率也许很小，也许是 $2^{-20000}$。但更有可能遇到有些方向的曲线会向上弯曲，另一些方向曲线向下弯，而不是所有的都向上弯曲，因此在高维度空间，更可能碰到鞍点。

| ![image](2.8-2%20局部最优之鞍点图.png) |
|----|

至于为什么会把一个曲面叫做鞍点，你想象一下，就像是放在马背上的马鞍一样，因此这里的这个点，导数为0的点，这个点叫做鞍点。

所以我们对低维度空间的大部分直觉，比如你可以画出上面的图，并不能应用到高维度空间中。适用于其它算法，因为如果你有2万个参数，那么成本函数 $J$ 有2万个维度向量，你更可能遇到鞍点，而不是局部最优点。

### 如果局部最优不是问题，那么问题是什么呢

结果是平稳段会减缓学习，平稳段是一块区域，其中导数长时间接近于0，如果你在一点，梯度会从曲面从上向下下降，因为梯度等于或接近0，曲面很平坦，你得花上很长时间慢慢抵达平稳段的这个点，因为左边或右边的随机扰动，因为左边或右边的随机扰动，然后你的算法能够走出平稳段。可以沿着这段长坡走，直到图上红色锐角顶点那里，然后走出平稳段。如下图所示：

| ![iamge](2.8-3%20局部最优之长坡平稳段.png) |
|----|

### 要点

1. **梯度不太可能困在极差的局部最优中**。条件是你在训练较大的神经网络，存在大量参数，并且成本函数 $J$ 被定义在较高的维度空间；
2. **平稳段使得学习十分缓慢**。这也是优化算法（如Momentum、RMSprop、Adam）能够加速学习算法的地方。在这些情况下，更成熟的优化算法，能够加快速度，让你尽早往下走出平稳段。
